#!/usr/bin/env python# -*- coding: utf-8 -*-"""-----------------------------------------@Created: 2020/12/28------------------------------------------@Modify: 2020/12/28------------------------------------------@Description:"""import hashlibimport osimport sysimport timefrom datetime import datetimefrom typing import Optional, Tuple, List, Callablefrom urllib.parse import urlparsefrom requests import Response_PROJECT_NAME = 'VulnerabilityAnalysis'_CURRENT_ABSPATH = os.path.abspath(__file__)sys.path.insert(0, _CURRENT_ABSPATH[:_CURRENT_ABSPATH.find(_PROJECT_NAME) + len(_PROJECT_NAME) + 1])# from vulnerability_analysis.utility import mysql_db_util as dbfrom vulnerability_analysis.utility.crawler import crawl_utilfrom vulnerability_analysis.utility.crawler.crawl_util import request_url_for_common_simplified, request_urltry: from vulnerability_analysis.utility.crawler import selenium_crawler_chrome_2except:    class FakeChrome:        @staticmethod        def quitCrawler(): pass        @staticmethod        def crawl_with_decode_and_status_code(url: str):            request_url_for_common_simplified(url)    selenium_crawler_chrome_2 = FakeChrome_URL_MAXSIZE = 3090_MEDIUMTEXT_MAXSIZE = 16777215 - 1# TableTB_CVE_REF = 'cve_ref'# 三种爬虫方法 (对应三个爬虫函数)METHOD_SYNC = 'sync'                # _crawl_sync(url)METHOD_ASYNC = 'async'              # _crawl_async(url)METHOD_GITHUB_API = 'github_api'    # _crawl_github_api(url)class CrawlerEntity:    def __init__(self, crawler_id: Optional[int], url: str, status_code: int, crawler_method: str,                 update_date: Optional[str], raw: str):        self.id = crawler_id        self.url = url        self.status_code = status_code        self.crawler_method = crawler_method        self.update_date = update_date if update_date is not None else _get_cur_time_mysql_style()        if raw is not None and len(raw) > _MEDIUMTEXT_MAXSIZE: raw = raw[:_MEDIUMTEXT_MAXSIZE]        self.raw = raw    def __str__(self) -> str:        return """id: %surl: '%s'status_code: %scrawler_method: %supdate_date: %sraw: %s""" % self.tup_all()    def tup_all(self): return self.id, self.url, self.status_code, self.crawler_method, self.update_date, self.rawdef quit_selenium_crawler():    """关闭selenium_crawler，减少进程driver开销"""    selenium_crawler_chrome_2.quitCrawler()def get(table: str, url: str, crawler_method: Optional[Callable[[str], CrawlerEntity]] = None,        force_update: bool = False) -> Optional[CrawlerEntity]:    """ 本模块唯一 API. 把网页缓存到数据库并返回. """    if len(url) > _URL_MAXSIZE: return _crawl_auto_choose_method(url, crawler_method)    if 'lists.apache.org/thread.html/' in url: url = url.replace('thread.html/', 'api/source.lua/')    if 'raw.githubusercontent.com' in url: url = url.replace('raw.githubusercontent.com', 'raw.fastgit.org')    ce = _db_get_without_raw_by_url(table, url)    if ce is None:        ce2 = _crawl_auto_choose_method(url, crawler_method)        _db_insert(table, ce2)        ce2.id = _db_get_without_raw_by_url(table, url).id        return ce2    if ce.crawler_method == METHOD_SYNC and crawler_method == A: force_update = True    if force_update:        ce2 = _crawl_auto_choose_method(url, crawler_method)        ce2.id = ce.id        _db_update(table, ce2)        return ce2    return _db_get_by_id(table, ce.id)def put_from_file(table: str, url: str, path: str, crawler_method_str: str = METHOD_ASYNC,        force_update: bool = True):    """ 不爬虫, 把已保存的网页文件保存至数据库. """    with open(path, 'r') as f: content = f.read()    ce = _db_get_without_raw_by_url(table, url)    if ce is None:        ce2 = CrawlerEntity(None, url, 200, crawler_method_str, None, content)        _db_insert(table, ce2)    elif force_update:        ce2 = CrawlerEntity(ce.id, url, 200, crawler_method_str, None, content)        _db_update(table, ce2)def put(table: str, url: str):    """ 调试用, 用于批量导入缓存. """    ce = _db_get_without_raw_by_url(table, url)    if ce is None:        print('[not in cache]')        _db_insert(table, _crawl_auto_choose_method(url))    else: print('[in cache]')def update(table: str, url: str):    """ 调试用, 用于批量更新缓存. """    ce = _db_get_without_raw_by_url(table, url)    if ce is None: print('[not in cache]')    else:        _db_update(table, _crawl_auto_choose_method(url))        print('[in cache]')def _db_build(table: str):    """ 建立个空表 (如果表不存在的话). """    sql = """CREATE TABLE IF NOT EXISTS `crawler`.`%s` (      `id` INT UNSIGNED NOT NULL AUTO_INCREMENT,      `url` VARCHAR(3090) NOT NULL,      `url_hash` BINARY(16) NOT NULL,      `status_code` SMALLINT DEFAULT NULL,      `crawler_method` VARCHAR(45) DEFAULT NULL,      `update_date` DATETIME DEFAULT NULL,      `raw` LONGTEXT,      PRIMARY KEY (`id`),      KEY `idx_url_hash` (`url_hash`)    ) ENGINE=InnoDB AUTO_INCREMENT=343872 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;""" % table    db.query(sql)    db.commit()def _db_get_by_id(table: str, uid: int):    """ 根据 ID 从数据库获取, 找不到则返回空. """    sql = """SELECT `id`, `url`, `status_code`, `crawler_method`, `update_date`, `raw` FROM `crawler`.`{}`        WHERE `id`=%s;""".format(table)    res: List[Tuple[int, str, int, str, str, str]] = db.query(sql, (uid,))    if len(res) == 0: return None    return CrawlerEntity(*res[0])def _db_get_by_url(table: str, url: str):    """ 根据 URL 从数据库获取, 找不到则返回空 (已根据 Hash 优化查询). """    sql = """SELECT `id`, `url`, `status_code`, `crawler_method`, `update_date`, `raw` FROM `crawler`.`{}`        WHERE `url_hash`=%s AND `url`=%s LIMIT 1;""".format(table)    res: List[Tuple[int, str, int, str, str, str]] = db.query(sql, (_url_hash(url), url))    if len(res) == 0: return None    return CrawlerEntity(*res[0])def _db_get_without_raw_by_url(table: str, url: str):    """ 根据 URL 从数据库获取, 找不到则返回空 (不获取 raw). """    sql = """SELECT `id`, `url`, `status_code`, `crawler_method`, `update_date` FROM `crawler`.`{}`        WHERE `url_hash`=%s AND `url`=%s LIMIT 1;""".format(table)    res: List[Tuple[int, str, int, str, str]] = db.query(sql, (_url_hash(url), url))    if len(res) == 0: return None    return CrawlerEntity(*res[0], None)def _db_insert(table: str, e: CrawlerEntity):    """ 插入新 URL 缓存 (不负责检查 URL 是否在表中). """    sql = """INSERT INTO `crawler`.`{}` (`url`, `url_hash`, `status_code`, `crawler_method`, `update_date`, `raw`)        VALUES (%s, %s, %s, %s, %s, %s);""".format(table)    db.query(sql, (e.url, _url_hash(e.url), e.status_code, e.crawler_method, e.update_date, e.raw))    db.commit()def _db_update(table: str, e: CrawlerEntity):    """ 根据 Entity 的 URL 更新数据库 (有 ID 就根据 ID 更新, 不然根据 URL). """    if e.id is not None:        sql = """UPDATE `crawler`.`{}` SET `status_code`=%s, `crawler_method`=%s, `update_date`=%s, `raw`=%s            WHERE `id`=%s;""".format(table)        db.query(sql, (e.status_code, e.crawler_method, e.update_date, e.raw, e.id))    else:        sql = """UPDATE `crawler`.`{}` SET `status_code`=%s, `crawler_method`=%s, `update_date`=%s, `raw`=%s            WHERE `url_hash`=%s AND `url`=%s LIMIT 1;""".format(table)        db.query(sql, (e.status_code, e.crawler_method, e.update_date, e.raw, _url_hash(e.url), e.url))    db.commit()def _crawl_auto_choose_method(url: str,                              crawler_method: Optional[Callable[[str], CrawlerEntity]] = None) -> CrawlerEntity:    """ 根据 URL 域名自动选择爬虫方法, 并返回爬取结果. """    if crawler_method is not None: return crawler_method(url)    return DOMAIN_METHOD.get(urlparse(url).netloc, _crawl_sync)(url)# def _crawl_sync(url: str) -> CrawlerEntity:#     """ 爬取 URL (同步, URL 返回的内容即全部). """#     status_code, raw = request_url_for_common_simplified(url)#     return CrawlerEntity(None, url, status_code, METHOD_SYNC, None, raw)def _crawl_sync(url: str) -> CrawlerEntity:    """ 爬取 URL (同步, URL 返回的内容即全部). """    # status_code, raw = request_url_for_common_simplified(url)    # return CrawlerEntity(None, url, status_code, METHOD_SYNC, None, raw)    reponse = crawl_util.request_url_for_common(url)  # option1， 不经过crawler DB    # if reponse: # option1， 不经过crawler DB    #     url_list, web_content = anlalyse_webcontent_util.extract_URLs_and_text(reponse.content, url)    return reponsedef _crawl_async(url: str) -> CrawlerEntity:    """ 爬取 URL (异步, 主要针对包含评论区等的 AJAX 内容). """    status_code, raw = selenium_crawler_chrome_2.crawl_with_decode_and_status_code(url)    return CrawlerEntity(None, url, status_code, METHOD_ASYNC, None, raw)def _crawl_github_api(url: str) -> CrawlerEntity:    """ 爬取 GITHUB API (需要 Token). """    res: Response = request_url(url)    if res is None:        time.sleep(30)        res = request_url(url)    raw = res.content.decode('utf-8')    status_code = res.status_code    return CrawlerEntity(None, url, status_code, METHOD_GITHUB_API, None, raw)def _url_hash(url: str) -> bytes:    return hashlib.md5(url.encode('utf-8')).digest()def _url_strip(url: str) -> str: return url[:3090]def _get_cur_time_mysql_style(): return datetime.now().strftime('%Y-%m-%d %H:%M:%S')def __insert_hash():    """ 计算每个 URL 的哈希 (MD5) 并插入字段. """    sql1 = """SELECT `id`, `url` FROM `crawler`.`cve_ref` WHERE `url_hash` IS NULL LIMIT 1000;"""    sql2 = """UPDATE `crawler`.`cve_ref` SET `url_hash`=CASE `id` %s END WHERE id IN (%s);"""    sql3 = """SELECT `id`, `url`, HEX(`url_hash`) FROM `crawler`.`cve_ref` WHERE `id`=%s;"""    # sql4 = """UPDATE `crawler`.`cve_ref` SET `url_hash`=%s WHERE id=%s;"""    while True:        todo_list: List[Tuple[int, str]] = db.query(sql1)        if len(todo_list) == 0: break        values = ' '.join(['WHEN %d THEN 0x%s' % (r[0], _url_hash(r[1]).hex()) for r in todo_list])        ids = ', '.join([str(r[0]) for r in todo_list])        db.query(sql2 % (values, ids))        db.commit()        print(db.query(sql3, (todo_list[-1][0],)))def __test():    # _db_build(TB_CVE_REF)    # url = 'https://lists.apache.org/thread.html/r36e44ffc1a9b365327df62cdfaabe85b9a5637de102cea07d79b2dbf@%3C' \    #       'commits.cxf.apache.org%3E '    # url = 'https://issues.apache.org/jira/browse/STORM-269'    # url = 'https://nvd.nist.gov/vuln/detail/CVE-2014-3682/cpes?expandCpeRanges=true'    # url = 'https://github.com/apache/hadoop'    # print(_crawl_sync(url))    # print(_crawl_async(url))    # print(get(TB_CVE_REF, url))    # print(get(TB_CVE_REF, url, crawler_method=A, force_update=True))    print('20ci')    t1 = time.time()    for _ in range(50):        _db_get_by_id(TB_CVE_REF, _db_get_without_raw_by_url(TB_CVE_REF, 'https://sourceforge.net/p/jyaml/bugs/41').id)    print(time.time() - t1)# 后期维护以下映射, 手动确定不同域名使用哪个方法爬取.S, A, G = _crawl_sync, _crawl_async, _crawl_github_apiDOMAIN_METHOD = {    'api.github.com': G,    'github.com': S,    'issues.apache.org': A}def test_insert_already_exists():    e = _db_get_by_id(TB_CVE_REF, 1)    _db_insert(TB_CVE_REF, e)if __name__ == '__main__':    # print(hashlib.md5(b'https://github.com/restify/node-restify/pull/1031').hexdigest())    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/506791', A, False))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/506791', A, True))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/793704', A, True))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/778414', A, True))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/526258', A, True))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/566056', A, True))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/343726', A, True))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/312918', A, True))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/243865', A, True))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/320693', A, True))    # print(get(TB_CVE_REF, 'https://hackerone.com/reports/54327', A, True))    # print(get(TB_CVE_REF, 'https://github.com/kubernetes/kubernetes/pull/72552', S, True))    # print( put_from_file( TB_CVE_REF,'https://github.com/kubernetes/kubernetes/pull/72552', '72552.html') )    # print(put_from_file(TB_CVE_REF, 'https://github.com/kubernetes/kubernetes/pull/66516', '66516.html'))    # print(put_from_file(TB_CVE_REF, 'https://github.com/FusionAuth/fusionauth-jwt/issues/3', '3.html'))    # print(get(TB_CVE_REF, 'http://git.imagemagick.org/repos/ImageMagick/blob/a01518e08c840577cabd7d3ff291a9ba735f7276', S, True))    # print(get(TB_CVE_REF, 'https://svn.apache.org/viewvc?view=rev&rev=781362'))    # https://git.libav.org/?p=libav.git;a=tree;h=23c79b45ba6ff76dda2b832cb7f59578b0f2b7ae    # https://svn.apache.org/viewvc?view=rev&rev=1521829    # https://svn.apache.org/viewvc?view=rev&rev=781362    # 周边出现 443情况，可能是由于爬虫被封；注意下，别错误识别为非200状态。    # __insert_hash()    'https://github.com/GoGentoOSS/SAMLBase/issues/3'    print(get(TB_CVE_REF, 'https://github.com/GoGentoOSS/SAMLBase/issues/3',A))