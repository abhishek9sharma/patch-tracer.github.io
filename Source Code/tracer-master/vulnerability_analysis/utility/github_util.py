#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
-----------------------------------------
@Created: 2020/10/09
------------------------------------------
@Modify: 2020/10/09
------------------------------------------
@Description:

用于实现Github相关操作:
 - request github sth
 - 根据git pull request -> git commit

"""

import random
import time
import os, sys, inspect
current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.append('../')
sys.path.append('../..')
sys.path.append('../../..')

from vulnerability_analysis import config
from vulnerability_analysis.utility import json_processing, log , file_processing, re_util
import time
import json
import requests
import urllib3
urllib3.disable_warnings()


import logging
logger = log.create_logger(level = logging.INFO, log_file_path = config.LOG_GITHUB_UTIL, logger_name = 'github_util', print_stream_cmd=True)

# SWITCH
# 查看是否已存在 gathered file, 有的话， 说明已爬取完，可直接pass
switch_check_gathered_commits_search_result =False

# 因为是直接copy，所以，函数内部变量没有改动，如：
# CVEID / DR010_SEACH_GITHUB_COMMITS_DIR / DR010_SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH
DR010_SEACH_GITHUB_COMMITS_DIR = config.DR010_SEACH_GITHUB_COMMITS_QUERY_DIR
DR010_SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH = config.DR010_SEACH_GITHUB_COMMITS_QUERY_MESSAGE_GATHERED_CONTENT_PATH

DR010_SEACH_GITHUB_CODE_QUERY_MESSAGE_DIR = config.DR010_SEACH_GITHUB_CODE_QUERY_MESSAGE_DIR
DR010_SEACH_GITHUB_CODE_QUERY_MESSAGE_GATHERED_CONTENT_PATH = config.DR010_SEACH_GITHUB_CODE_QUERY_MESSAGE_GATHERED_CONTENT_PATH

# git pull request -> git commit
# https://github.com/viabtc/viabtc_exchange_server/pull/131
def list_commits_on_a_pull_request( pull_request_url = str ):
    pull_request_commits_url = pull_request_url.strip('/') + '/' + 'commits'

    # create folder
    pull_request_folder_path = config.DR010_REQUEST_PULL_REQUESTS_DIR
    folder_name = pull_request_commits_url.split('/')[-5] + '__fdse__' + pull_request_commits_url.split('/')[-4] + '__fdse__' + pull_request_commits_url.split('/')[-2]
    file_name = 'commits.json'
    folder_path = pull_request_folder_path + folder_name
    file_path = pull_request_folder_path + folder_name + '/' + file_name
    file_processing.creatFolder_IfExistPass( folder_path )

    # 访问 / 保存内容
    if file_processing.pathExist( file_path ):
        commits_result = json_processing.read( file_path )
    else:
        commits_result = request_and_judge_response(pull_request_commits_url)
        if commits_result:
            json_processing.write(json_content=commits_result , path=file_path)
        return None


    # extract sth needed
    git_commits = []
    if not commits_result: None
    for ele in commits_result:
        commit_html_url = ele['html_url']
        git_commits.append(commit_html_url)

    return list(set(git_commits))

# git issue -> git commit
def list_commits_on_a_issue( issue_url = str):
    # 判断是git issue
    issue_url_events_url = issue_url.strip('/') + '/' + 'events'
    issueID = issue_url_events_url.split('/')[-2]
    if not str.isdigit(issueID): return None

    # create folder
    target_issue_folder_path = config.DR010_REQUEST_ISSUES_FOR_COMMITS_DIR
    folder_name = issue_url_events_url.split('/')[-5] + '__fdse__' + issue_url_events_url.split('/')[
        -4] + '__fdse__' + issue_url_events_url.split('/')[-2]
    file_name = 'events.json'
    folder_path = target_issue_folder_path + folder_name
    file_path = target_issue_folder_path + folder_name + '/' + file_name
    file_processing.creatFolder_IfExistPass(folder_path)

    # 访问 / 保存内容
    if file_processing.pathExist(file_path):
        events_result = json_processing.read(file_path)
    else:
        events_result = request_and_judge_response(issue_url_events_url)
        if events_result:
            json_processing.write(json_content=events_result, path=file_path)
        else:
            return None

    # extract sth needed
    git_commits = []
    if not events_result: return None
    for ele in events_result:
        commit_api_url = ele['commit_url']
        if commit_api_url:
            git_commits.append(commit_api_url)

    return list(set(git_commits))

def list_branches_of_a_repo(owner=str, repo=str, force_update=False):
    still_have_more_branches = True
    page = 1
    per_page = 100
    DR010_LIST_BRANCHES_OF_A_REPO_DIR = config.DR010_LIST_BRANCHES_OF_A_REPO_DIR
    DR010_LIST_BRANCHES_OF_A_REPO_GATHERED_CONTENT_PATH = config.DR010_LIST_BRANCHES_OF_A_REPO_GATHERED_CONTENT_PATH % (owner + "__fdse__" + repo)
    BRANCHES_OF_A_REPO_DIR = DR010_LIST_BRANCHES_OF_A_REPO_DIR + owner + "__fdse__" + repo + '/'

    # 爬取每个page
    while still_have_more_branches:
        page_path = BRANCHES_OF_A_REPO_DIR + 'page' + str(page) + '.json'
        if not force_update and file_processing.pathExist(page_path):
            events_result = json_processing.read( path=page_path )
        else:
            url = f'https://api.github.com/repos/{owner}/{repo}/branches?page={page}&per_page={per_page}'
            events_result = request_and_judge_response(url)
            file_processing.creatFolder_IfExistPass(BRANCHES_OF_A_REPO_DIR)  # 创建文件夹
            json_processing.write(events_result, page_path)
        if len(events_result) < 100: still_have_more_branches = False
        page += 1

    gathered_page_content = None
    page_list = page_list = file_processing.walk_L1_FileNames(BRANCHES_OF_A_REPO_DIR)
    # gathered_page_content
    for page in page_list:
        if not page.startswith('page'): continue
        page_path = BRANCHES_OF_A_REPO_DIR + str(page)
        page_content = json_processing.read(page_path)
        if not gathered_page_content:
            gathered_page_content = page_content
        else:
            gathered_page_content.extend( page_content )
    if gathered_page_content:
        json_processing.write(path=DR010_LIST_BRANCHES_OF_A_REPO_GATHERED_CONTENT_PATH, json_content=gathered_page_content)

    return gathered_page_content

def list_commits_with_parameters(owner=str, repo=str, branch=str, since=str, until=str, force_update=False):
    still_have_more_commits = True
    page = 1
    per_page = 100
    DR010_LIST_COMMITS_WITH_PARAMETERS_DIR = config.DR010_LIST_COMMITS_WITH_PARAMETERS_DIR
    DR010_LIST_COMMITS_WITH_PARAMETERS_GATHERED_CONTENT_PATH = config.DR010_LIST_COMMITS_WITH_PARAMETERS_GATHERED_CONTENT_PATH % (
                owner + "__fdse__" + repo + "__fdse__" + branch.replace('/','_') + "__fdse__" + since + "__fdse__" + until)
    COMMITS_WITH_PARAMETERS_DIR = DR010_LIST_COMMITS_WITH_PARAMETERS_DIR + owner + "__fdse__" + repo + "__fdse__" + branch.replace('/','_') + "__fdse__" + since + "__fdse__" + until + '/'

    # 爬取每个page
    while still_have_more_commits:
        page_path = COMMITS_WITH_PARAMETERS_DIR + 'page' + str(page) + '.json'
        if not force_update and file_processing.pathExist(page_path):
            try:
                events_result = json_processing.read(path=page_path)
            except json.decoder.JSONDecodeError:
                url = f'https://api.github.com/repos/{owner}/{repo}/commits?sha={branch}&page={page}&per_page={per_page}&since={since}&until={until}'
                events_result = request_and_judge_response(url)
                if events_result:
                    new_file_content = []
                    for item in events_result:
                        new_item = {}
                        new_item['html_url'] = item['html_url']
                        new_item['commit'] = item['commit']
                        new_file_content.append(new_item)
                    json_processing.write(json_content=new_file_content, path=page_path)
                else:
                    json_processing.write(events_result, page_path)
        else:
            url = f'https://api.github.com/repos/{owner}/{repo}/commits?sha={branch}&page={page}&per_page={per_page}&since={since}&until={until}'
            events_result = request_and_judge_response(url)
            file_processing.creatFolder_IfExistPass(COMMITS_WITH_PARAMETERS_DIR)  # 创建文件夹
            if events_result:
                new_file_content = []
                for item in events_result:
                    new_item = {}
                    new_item['html_url'] = item['html_url']
                    new_item['commit'] = item['commit']
                    new_file_content.append(new_item)
                json_processing.write(json_content=new_file_content, path=page_path)
            else:
                print( 'events_result',events_result )
                json_processing.write(events_result, page_path)
        # events_resu 结果有问题，简单排查

        if not events_result:
            if events_result == False:
                # 很多时候，就是404，没有结果；为了节省时间，不再requests
                # url = f'https://api.github.com/repos/{owner}/{repo}/commits?sha={branch}&page={page}&per_page={per_page}&since={since}&until={until}'
                # events_result = request_and_judge_response(url)
                # file_processing.creatFolder_IfExistPass(COMMITS_WITH_PARAMETERS_DIR)  # 创建文件夹
                # json_processing.write(events_result, page_path)
                print('events_result==False', page_path)
                # print('events_result==False', url)
            else:
                still_have_more_commits = False
        # 结果判断
        # 1  events 结果正确，且长度达到100， 才要继续requests；
        # 2 我们只取前10页，即：1k commit；（讲道理，有可能会丢失数据的）
        # if not (events_result and len(events_result) > 100):
        if not (events_result and len(events_result) >= 100) or page > 9:
            still_have_more_commits = False
        page += 1

    gathered_page_content = None
    page_list = file_processing.walk_L1_FileNames(COMMITS_WITH_PARAMETERS_DIR)
    # gathered_page_content
    for page in page_list:
        if not page.startswith('page'): continue
        page_path = COMMITS_WITH_PARAMETERS_DIR + str(page)
        try:
            page_content = json_processing.read(page_path)
        except json.decoder.JSONDecodeError:
            # todo, 先暂时pass了
            page_content = False
        if page_content==False or page_content==None: continue
        if not gathered_page_content:
            gathered_page_content = page_content
        else:
            gathered_page_content.extend(page_content)
    if gathered_page_content:
        # 减少空间存储
        # json_processing.write(path=DR010_LIST_COMMITS_WITH_PARAMETERS_GATHERED_CONTENT_PATH,
        #                       json_content=gathered_page_content)
        pass
    else:
        gathered_page_content = []
    return gathered_page_content


# 在GitHub中搜索 commit； 拼接并保存搜索结果
def query_github_for_commits(query_message, force_update=False) -> None:
    CVEID = query_message # 只是很多
    SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH = DR010_SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH % CVEID


    # 判断已爬取否
    existed_flag = False
    if not force_update and switch_check_gathered_commits_search_result and  file_processing.pathExist( SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH  ):
        existed_flag = True

    if not existed_flag:
        page_path = DR010_SEACH_GITHUB_COMMITS_DIR + CVEID + '/' + 'page%s.json' % str(1)
        url1 = 'https://api.github.com/search/commits?q=' + CVEID + '&page=' + str(1) + '&per_page=100'
        if not force_update and file_processing.pathExist(page_path  ):
            # file_content = json_processing.read( path = page_path)
            # response_page1 = file_content[CVEID]
            try:
                file_content = json_processing.read( path = page_path)
                response_page1 = file_content[CVEID]
            except json.decoder.JSONDecodeError:
                response_page1 = request_and_judge_response(url1)  # 待后期封装
                # 爬取第一个，并统计数据总量
                response_content_json = {}
                response_content_json[CVEID] = response_page1
                new_dir = DR010_SEACH_GITHUB_COMMITS_DIR.replace("\\", "/")
                file_processing.creatFolder_IfExistPass(new_dir + CVEID)  # 创建cve_id文件夹
                json_processing.write(response_content_json, page_path)
        else:
            response_page1 = request_and_judge_response(url1)  # 待后期封装
            # 爬取第一个，并统计数据总量
            response_content_json = {}
            response_content_json[CVEID] = response_page1
            new_dir=DR010_SEACH_GITHUB_COMMITS_DIR.replace("\\","/")
            file_processing.creatFolder_IfExistPass(new_dir + CVEID)  # 创建cve_id文件夹
            json_processing.write(response_content_json, page_path)

        total_count = response_page1['total_count']
        # print(total_count)
        if total_count > 999:
            logger.debug( CVEID + 'more than 1k!')
            # print(CVEID , 'more than 1k! just download top1k!')
            total_count = 999
            # return

        index = 1
        while total_count / 100 > 1:
            total_count = total_count - 100
            index = index + 1
            # 判断是否已爬取
            page_path = DR010_SEACH_GITHUB_COMMITS_DIR + CVEID + '/' + 'page%s.json' % str(index)
            if not force_update and file_processing.pathExist(page_path):
                continue
            else:
                url = 'https://api.github.com/search/commits?q=' + CVEID + '&page=' + str(index) + '&per_page=100'
                response_page = request_and_judge_response(url)
                #
                response_content_json = {}
                response_content_json[CVEID] = response_page
                file_processing.creatFolder_IfExistPass(DR010_SEACH_GITHUB_COMMITS_DIR + CVEID)  # 创建cve_id文件夹
                json_processing.write(response_content_json, page_path)
                #

    # 整合所有page内容
    cve_dir = CVEID
    page_list = file_processing.walk_L1_FileNames(DR010_SEACH_GITHUB_COMMITS_DIR + cve_dir)
    gathered_page_content = None
    # gathered_page_content
    for page in page_list:
        if not page.startswith('page'): continue

        page_content = json_processing.read(DR010_SEACH_GITHUB_COMMITS_DIR + cve_dir + '/' + page)
        if not gathered_page_content:
            gathered_page_content = page_content
        else:
            gathered_page_content[CVEID]['items'].extend(page_content[CVEID]['items'])
    if gathered_page_content:
        json_processing.write(path=SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH,  json_content= gathered_page_content)

        result_commits = []
        for record_item in gathered_page_content[CVEID]['items']:
            repo_full_name = record_item['repository']['full_name']
            html_url = record_item['html_url']
            result_commits.append(html_url)
        return result_commits

# 在GitHub中搜索 commit； 拼接并保存搜索结果
def query_github_for_commits_return_all_items(query_message) -> None:
    CVEID = query_message # 只是很多
    SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH = DR010_SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH % CVEID

    # 判断已爬取否
    existed_flag = False
    if switch_check_gathered_commits_search_result and file_processing.pathExist( SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH  ):
        existed_flag = True

    if not existed_flag:
        url1 = 'https://api.github.com/search/commits?q=' + CVEID + '&page=' + str(1) + '&per_page=100'
        response_page1 = request_and_judge_response(url1)  # 待后期封装
        #
        response_content_json = {}
        response_content_json[CVEID] = response_page1
        file_processing.creatFolder_IfExistPass(DR010_SEACH_GITHUB_COMMITS_DIR + CVEID)  # 创建cve_id文件夹
        json_processing.write(response_content_json, DR010_SEACH_GITHUB_COMMITS_DIR + CVEID +
                              '/page' + str(1) + '.json')
        #
        total_count = response_page1['total_count']
        print(total_count)
        if total_count > 999:
            logger.info( CVEID + 'more than 1k!')
            print(CVEID , 'more than 1k!')
            total_count = 999
            # return

        index = 1
        while total_count / 100 > 1:
            total_count = total_count - 100
            index = index + 1
            # 判断是否已爬取
            page_path = DR010_SEACH_GITHUB_COMMITS_DIR + CVEID + '/' + 'page%s.json' % str(index)
            if file_processing.pathExist(page_path):
                continue
            else:
                url = 'https://api.github.com/search/commits?q=' + CVEID + '&page=' + str(index) + '&per_page=100'
                response_page = request_and_judge_response(url)
                #
                response_content_json = {}
                response_content_json[CVEID] = response_page
                file_processing.creatFolder_IfExistPass(DR010_SEACH_GITHUB_COMMITS_DIR + CVEID)  # 创建cve_id文件夹
                json_processing.write(response_content_json, DR010_SEACH_GITHUB_COMMITS_DIR + CVEID +
                                      '/page' + str(index) + '.json')
                #

    # 整合所有page内容
    cve_dir = CVEID
    page_list = file_processing.walk_L1_FileNames(DR010_SEACH_GITHUB_COMMITS_DIR + cve_dir)
    gathered_page_content = None
    # gathered_page_content
    for page in page_list:
        if not page.startswith('page'): continue

        page_content = json_processing.read(DR010_SEACH_GITHUB_COMMITS_DIR + cve_dir + '/' + page)
        if not gathered_page_content:
            gathered_page_content = page_content
        else:
            gathered_page_content[CVEID]['items'].extend(page_content[CVEID]['items'])
    if gathered_page_content:
        json_processing.write(path=SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH,  json_content= gathered_page_content)

        # result_commits = []
        # for record_item in gathered_page_content[CVEID]['items']:
        #     repo_full_name = record_item['repository']['full_name']
        #     html_url = record_item['html_url']
        #     result_commits.append(html_url)
        return gathered_page_content[CVEID]['items']

# request specific commits
def request_specific_commit(owner,repo,commitID):
    raw_commit_path = config.DR010_GIT_COMMITS_META_RAW_DIR + commitID + ".json"
    if file_processing.pathExist(path=raw_commit_path):
        try:
            res =  json_processing.read( path=raw_commit_path ) # 文件有误
            # if not res: # 2021.04.29,只跑着一次，因为有些就是4040，是没有的
            #     url = config.URL_GITHUB_COMMIT % (owner, repo, commitID)
            #     commit_info_json = request_and_judge_response(url)
            #     json_processing.write(json_content=commit_info_json, path=raw_commit_path)
            return res
        except json.decoder.JSONDecodeError:
            url = config.URL_GITHUB_COMMIT % (owner, repo, commitID)
            commit_info_json = request_and_judge_response(url)
            json_processing.write(json_content=commit_info_json, path=raw_commit_path)
            return commit_info_json
    else:
        url = config.URL_GITHUB_COMMIT % (owner,repo,commitID)
        commit_info_json = request_and_judge_response(url)
        json_processing.write(json_content=commit_info_json, path=raw_commit_path )
        return commit_info_json

def is_valid_github_commit_url(url):
    """ 若不是，则返回false """
    separate_url_list = url.split("/")
    if len(separate_url_list) < 7: return None
    owner = separate_url_list[3]
    repo = separate_url_list[4]
    commitID = re_util.extract_commitid_in_commit_url(url)
    if not commitID or len(commitID)<6: return False
    # 获取文件路径
    raw_commit_path = config.DR010_GIT_COMMITS_META_RAW_DIR + commitID + ".json"
    if file_processing.pathExist(path=raw_commit_path):
        try:
            res =  json_processing.read( path=raw_commit_path ) # 文件有误
            return res
        except json.decoder.JSONDecodeError:
            url = config.URL_GITHUB_COMMIT % (owner, repo, commitID)
            commit_info_json = request_and_judge_response(url)
            json_processing.write(json_content=commit_info_json, path=raw_commit_path)
            return commit_info_json
    else:
        url = config.URL_GITHUB_COMMIT % (owner,repo,commitID)
        commit_info_json = request_and_judge_response(url)
        json_processing.write(json_content=commit_info_json, path=raw_commit_path )
        return commit_info_json


headers_option = 0

# request
def request_url(url):
    global headers_option
    # print('------------------------- url:' + url)
    logger.info('------------------------- url:' + url)
    if headers_option > -1:
        # headers = {'User-Agent': random.choice(user_agents.agents) , "Accept-Language": 'zh-CN,zh;q=0.9,en;q=0.8'}
        headers = {'Authorization': 'token 336edd76722aa1421c0370128a8241dd486cdd28', "Accept-Language": 'q=0.9,en;q=0.8',
                   'Accept': 'application/vnd.github.cloak-preview+json'} # 3rdpartylib
    s = requests.session()
    s.keep_alive = False
    try:
        url_content = requests.get(url, headers=headers, verify=False, timeout=(18, 72))
    except Exception as e:
        print('Error' , e)
        logger.info('Error' + e.__str__())
        return None
    else:
        return url_content

def request_and_judge_response(url):
    global headers_option
    time.sleep(1)  # 防止爬虫频率过高
    # url = 'https://api.github.com/search/issues?q=' + cve_id + '&page=' + str(page_num) + '&per_page=100'
    # request and save
    while True:
        try:
            response = request_url(url)
            if response:
                logger.debug(response.headers['X-RateLimit-Limit'])
                logger.debug(response.headers['X-RateLimit-Remaining'])
                logger.info("response.headers['X-RateLimit-Limit']" + response.headers[
                    'X-RateLimit-Limit'].__str__())
                logger.info("response.headers['X-RateLimit-Remaining']" + response.headers[
                    'X-RateLimit-Remaining'].__str__())
                remian_count = int( response.headers['X-RateLimit-Remaining'] )
                if remian_count< 300:
                    print(headers_option)
                    headers_option = (headers_option + 1)%6
                    print(headers_option)
                break
            elif response.status_code == 404:
                logger.info('response.status_code == 404, ' + url.__str__())
                return False
            elif response.status_code == 400: # url参数无效
                logger.info('response.status_code == 400, ' + url.__str__())
                return False
            elif response.status_code == 422:
                # 422, 是没找到，如：https://api.github.com/repos/spring-projects/spring-framework/commits/03f547
                logger.info('response.status_code == 422, ' + url.__str__())
                return False
            elif response.status_code == 410:
                logger.info('response.status_code == 410, ' + url.__str__())
                return False
            elif response.status_code == 403:
                logger.info('response.status_code == 403, ' + url.__str__())
                # 切换headers_option， 继续
                print(headers_option)
                headers_option = (headers_option + 1) % 6
                print(headers_option)
            else:  # 爬虫出错
                logger.info('Try again, sth wrong? 1!')
                logger.info('response.status_code：' + response.status_code.__str__())
                time.sleep(30)

        except Exception as e:
            logger.info('Try again, sth wrong? 2!' + e.__str__())
            logger.debug( e )
            time.sleep(30)

            # 切换headers_option， 继续
            print(headers_option)
            headers_option = (headers_option + 1) % 6
            print(headers_option)
            continue

    issue_list = {}
    print(response.status_code)
    if response.status_code == 200:  # 成功
        response_content = json.loads(response.text)
        return response_content
    elif response.status_code == 400 or response.status_code == 404 or response.status_code == 500:  # 特殊处理一下
        # 被封处理
        logger.info('request:' + url + ', status: ' + str(response.status_code))
        logger.info('sleep 30s')
        time.sleep(30)  # 被封后，停30s再试一次
        response = request_url(url)
        if response.status_code == 200:
            response_content = json.loads(response.text)
            return response_content
        else:
            return False
    else:
        # 被封处理
        logger.info('request:' + url + ', status: ' + str(response.status_code))
        for count in range(3):
            logger.info('sleeping, ' + count.__str__() + 'mins')
            time.sleep(60 * 10)
        request_and_judge_response(url)

# convert api url to html url
def convert_api_url_to_html_url(url_to_convert):
    formal_url = url_to_convert
    if 'api.github.com/repos' in url_to_convert and '/commits/' in url_to_convert:
        formal_url = url_to_convert.replace('api.github.com/repos', 'github.com').replace('/commits/', '/commit/')
    return formal_url

def search_code(query_message):
    # todo
    """
    依据query_message， 检索相关的文件内容
    :param query_message:
    :return:
    """
    # 因为是直接copy，所以，函数内部变量没有改动，如下：
    CVEID = query_message
    SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH = DR010_SEACH_GITHUB_CODE_QUERY_MESSAGE_GATHERED_CONTENT_PATH % CVEID
    DR010_SEACH_GITHUB_COMMITS_DIR = DR010_SEACH_GITHUB_CODE_QUERY_MESSAGE_DIR

    # 判断已爬取否
    existed_flag = False
    if switch_check_gathered_commits_search_result and file_processing.pathExist(
            SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH):
        existed_flag = True

    if not existed_flag:
        page_path = DR010_SEACH_GITHUB_COMMITS_DIR + CVEID + '/' + 'page%s.json' % str(1)
        url1 = 'https://api.github.com/search/code?q=' + CVEID + '&page=' + str(1) + '&per_page=100'
        if file_processing.pathExist(page_path):
            file_content = json_processing.read(path=page_path)
            response_page1 = file_content[CVEID]
        else:
            response_page1 = request_and_judge_response(url1)  # 待后期封装
            # 爬取第一个，并统计数据总量
            response_content_json = {}
            response_content_json[CVEID] = response_page1
            file_processing.creatFolder_IfExistPass(DR010_SEACH_GITHUB_COMMITS_DIR + CVEID)  # 创建cve_id文件夹
            json_processing.write(response_content_json, page_path)

        total_count = response_page1['total_count']
        print(total_count)
        if total_count > 999:
            logger.debug(CVEID + 'more than 1k!')
            # print(CVEID, 'more than 1k! just download top1k!')
            total_count = 999
            # return

        index = 1
        while total_count / 100 > 1:
            total_count = total_count - 100
            index = index + 1
            # 判断是否已爬取
            page_path = DR010_SEACH_GITHUB_COMMITS_DIR + CVEID + '/' + 'page%s.json' % str(index)
            if file_processing.pathExist(page_path):
                continue
            else:
                url = 'https://api.github.com/search/commits?q=' + CVEID + '&page=' + str(index) + '&per_page=100'
                response_page = request_and_judge_response(url)
                #
                response_content_json = {}
                response_content_json[CVEID] = response_page
                file_processing.creatFolder_IfExistPass(DR010_SEACH_GITHUB_COMMITS_DIR + CVEID)  # 创建cve_id文件夹
                json_processing.write(response_content_json, page_path)
                #

    # 整合所有page内容
    cve_dir = CVEID
    page_list = file_processing.walk_L1_FileNames(DR010_SEACH_GITHUB_COMMITS_DIR + cve_dir)
    gathered_page_content = None
    # gathered_page_content
    for page in page_list:
        if not page.startswith('page'): continue

        page_content = json_processing.read(DR010_SEACH_GITHUB_COMMITS_DIR + cve_dir + '/' + page)
        if not gathered_page_content:
            gathered_page_content = page_content
        else:
            gathered_page_content[CVEID]['items'].extend(page_content[CVEID]['items'])
    if gathered_page_content:
        json_processing.write(path=SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH, json_content=gathered_page_content)

        result_commits = []
        for record_item in gathered_page_content[CVEID]['items']:
            repo_full_name = record_item['repository']['full_name']
            html_url = record_item['html_url']
            result_commits.append(html_url)
        return result_commits
    pass

def query_github_for_commits_test():
    # query_message = '0470'
    # query_github_for_commits('123276',force_update=True)
    # query_github_for_commits('0470')
    query_github_for_commits('12896',force_update=True)

if __name__ == '__main__':
    # res = list_commits_with_parameters(owner='rails', repo='rails', branch='3-0-stable', since='2011-08-16T00:00:00Z',
    #                                    until='2011-08-16T23:59:59Z')
    # print( len(res) )
    # print( res )
    # res = list_commits_with_parameters(owner='rails', repo='rails', branch='3-0-stable', since='2011-08-13T22:24:48Z', until='2011-08-19T22:24:48Z')
    # print( len(res) )
    # res = list_branches_of_a_repo( owner='apache' , repo='lucene-solr' )
    # print(len(res))
    # res = list_branches_of_a_repo(owner='rails', repo='rails')
    # print(len(res))

    # query_github_for_commits_test()
    # test
    # request_and_judge_response( 'https://api.github.com/repos/jgraph/mxgraph/issues/124/events'  )

    # url = 'https://github.com/zkat/ssri/commit/d0ebcdc22cb5c8f47f89716d08b3518b2485d65d'
    # res = is_valid_github_commit_url(url)
    # print(res)
    url = 'https://github.com/chakra-core/ChakraCore/commit/9f1046569d2e382'
    url = 'https://github.com/request/request/commit/33286a994b018c279e687623791c2e672b500'
    res = is_valid_github_commit_url(url)
    print(res)



