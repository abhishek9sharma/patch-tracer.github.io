#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
-----------------------------------------
@Created: 2020/09/03
------------------------------------------
@Modify: 2020/09/03
------------------------------------------
@Description:


用于解决功能：request某个url，获取并解析该网页。
提取：
 - 网页信息，用于判断是否含有diff
 - 网页内，url信息，用于判断是否有 issue / commit 等链接
"""

import logging
import os, sys, inspect
current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.append('../')
sys.path.append('../..')
sys.path.append('../../..')


from vulnerability_analysis.utility import json_processing, log, file_processing
from vulnerability_analysis.utility.crawler import crawl_util
from vulnerability_analysis import config
import nltk
import json, time
import re

from bs4 import BeautifulSoup

from urllib.parse import urlparse
from urllib.parse import urljoin
from urllib.parse import urlunparse
from posixpath import normpath
from vulnerability_analysis.utility.crawler import crawler_db as cdb
from vulnerability_analysis.utility.crawler import selenium_crawler_chrome_2

BLACK_DOMAIN_LIST = ['rhn.redhat.com/errata' , 'redhat.com/support' , 'securityfocus.com/bid' , 'osvdb.org/' , 'exchange.xforce.ibmcloud.com',
                     'bugzilla.novell.com/' , 'linkedin.com/' , 'fb.com/']

# 识别 text中的diff 信息
def identify_diff_text(content) -> None:
    ## identify 'diff pattern'
    if '---' in content and '+++' in content and '@@' in content:
        # TODO, 用正则将 diff部分提取出来～
        return True
    else:
        return None

# 获取网页，并识别出 diff类型的信息
def request_and_identify_diff_text(url) -> None:
    # 黑名单， black_domain_list
    for black_domian in BLACK_DOMAIN_LIST:
        if black_domian in url:
            return None

    html = _request_web(url)
    if not html: # request 报错，返回为空
        return None
    try:
        soup = BeautifulSoup(html, 'html.parser')
    except TypeError:
        print('ERROR when parsing: ', url)
        return None
    ## get_text
    html_text = soup.get_text()

    ## identify 'diff pattern'
    if '---' in html_text and '+++' in html_text and '@@' in html_text:
        # TODO, 用正则将 diff部分提取出来～
        return True
    else:
        return None

# 获取网页，并识别出 diff类型的信息
def request_and_identify_diff_text_via_crawler_DB(url) -> None:
    # 黑名单， black_domain_list
    for black_domian in BLACK_DOMAIN_LIST:
        if black_domian in url:
            return None

    reponse_entity_from_DB = cdb.get(table='cve_ref', url=url, crawler_method=cdb.S)
    html = reponse_entity_from_DB.raw
    # html = selenium_crawler_chrome_2.getHtmlFromUrl(url)
    if not html: # request 报错，返回为空
        return None
    try:
        soup = BeautifulSoup(html, 'html.parser')
    except TypeError:
        print('ERROR when parsing: ', url)
        return None
    ## get_text
    html_text = soup.get_text()

    ## identify 'diff pattern'
    if '---' in html_text and '+++' in html_text and '@@' in html_text:
        # TODO, 用正则将 diff部分提取出来～
        return True
    else:
        return None


# # 获取网页，并识别出 commit / other type url
# def request_and_indentify_URLs(url):
#     url_list = request_and_extract_URLs(url)
#     pass

# 输入网页中的某个标签对象，并从中提取url信息; 以及该网页的文本信息, get_text()
def extract_URLs_and_text_from_soup_tag(soup_obj,original_url):
    url = original_url
    html_text = soup_obj.get_text()
    url_list = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', html_text)
    url_list = [ele + '__fdse__' + 'from html content' for ele in url_list]

    ## collect <a> tag
    a_list = soup_obj.find_all('a')
    # print(a_list)
    for ele in a_list:
        try:
            part_url = ele['href']
            desc_url = ele.text
            full_url = _url_join(base=url, url=part_url)
            # if 'patch' in ele.text:
            #     print('12')
            url_list.append(full_url + '__fdse__' + desc_url.strip('\n'))
            # print('url', url)
            # print('part_url', part_url)
            # print('full_url', full_url + '__fdse__' + desc_url.strip('\n'))
        except KeyError:
            # print('keyerror',ele)
            continue
    return list(set(url_list)), html_text

# 输入网页，并从中提取url信息; 以及该网页的文本信息, get_text()
def extract_URLs_and_text(html,original_url):
    url = original_url
    soup = BeautifulSoup(html, 'html.parser')
    ## get_text
    html_text = soup.get_text()
    url_list = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', html_text)
    url_list = [ele + '__fdse__' + 'from html content' for ele in url_list]

    ## collect <a> tag
    a_list = soup.find_all('a')
    # print(a_list)
    for ele in a_list:
        try:
            part_url = ele['href']
            desc_url = ele.text
            full_url = _url_join(base=url, url=part_url)
            # if 'patch' in ele.text:
            #     print('12')
            url_list.append(full_url + '__fdse__' + desc_url.strip('\n'))
            # print('url', url)
            # print('part_url', part_url)
            # print('full_url', full_url + '__fdse__' + desc_url.strip('\n'))
        except KeyError:
            # print('keyerror',ele)
            continue
    return list(set(url_list)), html_text

# 输入网页，并从中提取urls、issuekey、commitID信息
def extract_URLs_issueKey_commitID_from_text(html,original_url):
    """ 注意：issuekey、commitID， 不出现在URL中 """
    url = original_url
    soup = BeautifulSoup(html, 'html.parser')
    ## get_text
    html_text = soup.get_text()
    url_list = re.findall('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', html_text)
    url_list = [ele + '__fdse__' + 'from html content' for ele in url_list]

    ## collect <a> tag
    a_list = soup.find_all('a')
    # print(a_list)
    for ele in a_list:
        try:
            part_url = ele['href']
            desc_url = ele.text
            full_url = _url_join(base=url, url=part_url)
            # if 'patch' in ele.text:
            #     print('12')
            url_list.append(full_url + '__fdse__' + desc_url.strip('\n'))
            # print('url', url)
            # print('part_url', part_url)
            # print('full_url', full_url + '__fdse__' + desc_url.strip('\n'))
        except KeyError:
            # print('keyerror',ele)
            continue

    url_list_str = ('').join([ele.split('__fdse__')[0] for ele in url_list])
    # 只识别 'JIRA-1234' 有字母和数据的issuekey
    issuekey_list = re.findall('[A-Z]+-[0-9]+', html_text)
    issuekey_list = [ele for ele in issuekey_list if ele not in url_list_str ]
    commitID_list = re.findall('(?<![0-9a-f])(?![0-9]{6,40})(?![a-f]{6,40})([0-9a-f]{6,40})(?![0-9a-f])', html_text)
    commitID_list = [ele for ele in commitID_list if ele not in url_list_str ]
    return list(set(url_list)), list(set(issuekey_list)), list(set(commitID_list))

def request_and_extract_URLs_issueKey_commitID_from_text(url):
    reponse_entity_from_DB = cdb.get(table='cve_ref', url=url, crawler_method=cdb.S)
    html = reponse_entity_from_DB.raw
    if not html: # request 报错，返回为空
        return None
    return extract_URLs_issueKey_commitID_from_text(html, url)

# 获取网页，并从中提取url信息; 以及该网页的文本信息, get_text()
def request_and_extract_URLs_and_text(url):
    html = _request_web(url)
    return extract_URLs_and_text(html,url)

# 获取网页信息
def _request_web(url):
    # html = crawl_util.request_url(url).text
    html = crawl_util.request_and_judge_response(url)
    if not html:
        return None
    else:
        return html

# 拼接url
# https://www.cnblogs.com/shijiaoyun/p/4863813.html
# https://www.cnblogs.com/itlqs/p/6055365.html
def _url_join(base, url):
    try:
        url1 = urljoin(base, url)
    except ValueError:
        print('ValueError: urljoin(base, url) ',base, url)
        return base
    arr = urlparse(url1)
    path = normpath(arr[2])
    return urlunparse((arr.scheme, arr.netloc, path, arr.params, arr.query, arr.fragment))

if __name__ == '__main__':
    # test
    base = 'https://sourceware.org/bugzilla/show_bug.cgi?id=23787'
    url = 'https://www.shoesformen.com'
    print( _url_join(base, url) )

# url = 'https://www.openwall.com/lists/oss-security/2016/06/20/2'
# print( request_and_extract_URLs(url) )

# url = 'https://www.openwall.com/lists/oss-security/2016/06/20/2/1'
# print( request_and_identify_diff_text(url) )

# url = 'http://www.sourceforge.net/tracker/?func=detail&atid=100103&aid=474616&group_id=103'
# print( request_and_identify_diff_text(url) )

# url = 'https://bugzilla.redhat.com/attachment.cgi?id=154896'
# print(request_and_identify_diff_text(url))

# url = 'https://bugzilla.mozilla.org/show_bug.cgi?id=477979'
# res = request_and_extract_URLs_and_text(url)
# print(res)

# url = 'https://git.openssl.org/gitweb?p=openssl.git;a=snapshot;h=2b0532f3984324ebe1236a63d15893792384328d;sf=tgz'
# res = request_and_identify_diff_text(url)
# print(url)

# url = 'https://fb.com/symbiansymohTwitter'
# url = 'https://ohler.com/ox'
# url = 'https://download.pydio.com/pub/cells/release/1.5.0/darwin-amd64/cells'
# url = 'https://127.0.0.1:6379/?q=HTTP/1.1\r\nSE'
# res = request_and_identify_diff_text(url)
# print(res)
