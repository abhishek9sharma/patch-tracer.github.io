#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
-----------------------------------------
@Created: 2021/04/15
------------------------------------------
@Modify: 2021/04/15
------------------------------------------
@Description:

"""

import os,sys,re
_PROJECT_NAME = 'VulnerabilityAnalysis'
_MODULE_NAME = 'VulnerabilityAnalysis/vulnerability_analysis'
_CURRENT_ABSPATH = os.path.abspath(__file__)
sys.path.insert(0, _CURRENT_ABSPATH[:_CURRENT_ABSPATH.find(_PROJECT_NAME) + len(_PROJECT_NAME) + 1])
sys.path.insert(0, _CURRENT_ABSPATH[:_CURRENT_ABSPATH.find(_PROJECT_NAME) + len(_MODULE_NAME) + 1])
sys.path.insert(0, _CURRENT_ABSPATH)
os.environ['OPENBLAS_NUM_THREADS'] = '2'

sys.path.append('../')
sys.path.append('../..')
sys.path.append('../../..')
sys.path.append('../../../..')

from vulnerability_analysis import  config
from vulnerability_analysis.patch_localization.tool import cls_cve_localized_patch, cls_node
from vulnerability_analysis.patch_localization.tool.search_github import search_github_for_commits
from vulnerability_analysis.utility import debian_util, redhat_util, anlalyse_webcontent_util, parse_github_commit_util
from vulnerability_analysis.utility import re_util, github_util, log
from vulnerability_analysis.utility.crawler import crawl_util

import time
import pickle
# CVE-2017-16042

import logging
logger_sprcial_info = log.create_logger(level = logging.INFO, log_file_path = config.PATCH_LOCALICATION_OUTPUT_SPECIAL_LOG_PATH, logger_name = 'special_info', print_stream_cmd=True)


def read_NRD_reports_ref(CVEID, cve_patches_obj):
    src_refs = {}
    # N
    N_references = []
    if 'N' in cve_patches_obj.sources:
        if cve_patches_obj.CVE_item_content and cve_patches_obj.CVE_item_content != {}:
            N_references = cve_patches_obj.CVE_item_content['Reference']
            if N_references: src_refs['N'] = N_references
            else: src_refs['N'] = []
    # R
    if 'R' in cve_patches_obj.sources:
        R_references = redhat_util.get_refs_in_Redhat_report(CVEID=CVEID, force_update=True)
        if R_references: src_refs['R'] = R_references
        else: src_refs['R'] = []
    # D
    if 'D' in cve_patches_obj.sources:
        D_references = debian_util.get_refs_in_Debian_report(CVEID=CVEID)
        if D_references: src_refs['D'] = D_references
        else: src_refs['D'] = []

    # src_refs
    # NRD_refs = src_refs['N'] + src_refs['R'] + src_refs['D']
    return src_refs

def extract_urls_in_node_website(node: cls_node.Node):
    url_list = []
    ref_url = node.formatted_url

    reponse = crawl_util.request_url_for_common(ref_url)  # option1， 不经过crawler DB
    if reponse: # option1， 不经过crawler DB
        url_list, web_content = anlalyse_webcontent_util.extract_URLs_and_text(reponse.content, ref_url)

    # reponse_entity_from_DB = cdb.get(table='cve_ref', url=ref_url, crawler_method=cdb.S)  # option2， 基于crawler DB
    # if reponse_entity_from_DB.status_code != 200:  # 多试一次
    #     reponse_entity_from_DB = cdb.get(table='cve_ref', url=ref_url, crawler_method=cdb.S,
    #                                      force_update=True)  # option2， 基于crawler DB
    # if reponse_entity_from_DB and reponse_entity_from_DB.raw:  # option2， 基于crawler DB
    #     url_list, web_content = anlalyse_webcontent_util.extract_URLs_and_text(
    #         reponse_entity_from_DB.raw, ref_url)
    return url_list

def filter_identified_nodes(children_nodes_to_filter,parent_node):
    selected_nodes = []
    selected_nodes_id = [] # 用于去重
    parent_repo = parent_node.formatted_url.split('github.com/')[-1].split('/issues')[0].split('/pull')[0] # 因为可能是 issue / PR
    parent_repos = [parent_repo]
    # 由于项目owner可能会变更，如： https://github.com/kennethreitz/requests/issues/1885 == https://github.com/psf/requests/issues/1885
    # 所以， 先对 parent_repo 进行check
    if parent_node.type == 'issue_url' and parent_node.category_in_type in ['git_issue', 'git_pull_request']:
        onwer = parent_repo.split('/')[0]
        repo = parent_repo.split('/')[-1]
        issse_pr_num = parent_node.formatted_url.split('issues/')[-1].split('/pull')[-1].split('/')[0]
        for child_node in children_nodes_to_filter:
            if child_node.type == 'issue_url' and child_node.category_in_type in ['git_issue', 'git_pull_request']:
                child_repo = child_node.formatted_url.split('github.com/')[-1].split('/issues')[0].split('/pull')[0]
                child_repo_onwer = child_repo.split('/')[0]
                child_repo_repo = child_repo.split('/')[-1]
                child_repo_issse_pr_num = parent_node.formatted_url.split('issues/')[-1].split('/pull')[-1].split('/')[0]
                if child_repo_onwer != onwer and  child_repo_repo == repo and child_repo_issse_pr_num == issse_pr_num: #大概率进行了变更
                    parent_repos.append(child_repo)

    # 迭代
    for child_node in children_nodes_to_filter:
        flag_select = 1
        # 如果，父节点是issue， 子节点跳转至其他repo的issue/PR/commit， 则不保留
        if parent_node.type == 'issue_url' and parent_node.category_in_type in ['git_issue','git_pull_request']:
            child_repo = child_node.formatted_url.split('github.com/')[-1].split('/issues')[0].split('/pull')[0].split('/commit')[0]
            if child_repo not in parent_repos:
                flag_select = 0

        if flag_select==1 and child_node.formatted_id not in selected_nodes_id:
            selected_nodes.append(child_node)
            selected_nodes_id.append(child_node.formatted_id)
    return selected_nodes

def extract_and_filter_issueKey_gitrepo_info_in_nodes(node_list):
    # result = {'common_github_url':[], 'issue_url':[]}
    result = {'github_repo': [], 'issuekey': []}
    result = []
    for node in node_list:
        node_original_content = node.original_content # url or other
        # issuekey
        issuekey_list = re_util.extract_issuekey_from_url(url=node_original_content)
        issuekey_list = [ ele for ele in issuekey_list if not ele.startswith('CVE-')] # 'CVE-2018' 误报
        # result['issuekey'] += issuekey_list
        if issuekey_list:
            result.append( {'type':'issuekey', 'node_obj': node, 'issuekey_list': issuekey_list} )
        # 决定删去，因为噪音实在是太大了  https://github.com/CongyingXU/VulnerabilityAnalysis/issues/95
        # elif node.type == 'issue_url' and node.category_in_type not in ['git_issue', 'git_pull_request']: # 用于识别非JIRA-xxxx类型的issueid， 除去git issue/PR
        #     issuekey_list = re_util.extract_issueid_from_url(url=node_original_content)
        #     if issuekey_list:
        #         result.append({'type': 'issuekey', 'node_obj': node, 'issuekey_list': issuekey_list})
        # github_repo
        if 'github.com/' in node_original_content and len( node_original_content.split('github.com/')[-1].split('/') ) >1 :
            github_repo = ('/').join( node_original_content.split('github.com/')[-1].split('/')[:2] )
            # result['github_repo'].append(github_repo)
            result.append({'type': 'github_repo', 'node_obj': node, 'github_repo': github_repo})
    # result['github_repo'] = list(set( result['github_repo'] ))
    # result['issuekey'] = list(set(result['issuekey']))
    return result

def search_github_for_cve_fix(CVEID, issueKey_gitrepo_info, CVEID_node: cls_node):
    step60_time = time.time()
    result = []
    # github_repos = issueKey_gitrepo_info['github_repo']

    # todo, 指定repo时

    # 未指定github_repo时
    # CVEID for commit， todo
    CVEID_search_github_result = search_github_for_commits.main_CVEID_for_patch_collect(CVEID)
    # print('CVEID_search_github_result',CVEID_search_github_result)
    if CVEID_search_github_result and CVEID_search_github_result['git_commit']:
        result_item = {'input_type': 'CVEID', 'input_node': CVEID_node, 'input_content': CVEID,
                       'commits': CVEID_search_github_result['git_commit']}
        result.append(result_item)
    step61_time = time.time()
    print(CVEID,'step61_time - step60_time: ', step61_time - step60_time)
    # issuekey for commit
    for ele in issueKey_gitrepo_info:
        if not (ele['type']=='issuekey' and ele['issuekey_list']): continue
        issuekeys = ele['issuekey_list']
        for issuekey_str in issuekeys:
            issueKey_search_github_result = search_github_for_commits.main_issueKey_for_patch_collect(CVEID,issuekey_str, force_update=True)
            # print('issueKey_search_github_result',issueKey_search_github_result)
            if issueKey_search_github_result and issueKey_search_github_result['git_commit']:
                result_item = { 'input_type': 'issuekey', 'input_node': ele['node_obj'],'input_content': issuekey_str, 'commits': issueKey_search_github_result['git_commit']}
                result.append(result_item)

    # 返回关键词（父节点）及对应的commit结果（子节点），便于连线
    print(CVEID,'result: search_github_for_cve_fix ',result)
    return result

def write_cve_patch_result(cve_patches_obj):
    CVEID = cve_patches_obj.CVEID
    PATCH_LOCALICATION_OUTPUT_CVE_PATCH_OBJ_PATH = config.PATCH_LOCALICATION_OUTPUT_CVE_PATCH_OBJ_PATH % CVEID
    with open( PATCH_LOCALICATION_OUTPUT_CVE_PATCH_OBJ_PATH,'wb') as f:
        f.write( pickle.dumps(cve_patches_obj) )

def read_cve_patch_result(CVEID):
    cve_patches_obj = None
    PATCH_LOCALICATION_OUTPUT_CVE_PATCH_OBJ_PATH = config.PATCH_LOCALICATION_OUTPUT_CVE_PATCH_OBJ_PATH % CVEID
    with open(PATCH_LOCALICATION_OUTPUT_CVE_PATCH_OBJ_PATH, 'rb') as f:
        return pickle.loads( f.read() )

def visualize_and_save_graph_fig(CVEID):
    # 由于并发时，save fig会引入fig数据错乱的问题
    cve_patches_obj = read_cve_patch_result(CVEID)
    # # 可视化
    save_fig_path = config.PATCH_LOCALICATION_OUTPUT_CVE_GRAPH_PATH % CVEID
    cve_patches_obj.url_graph.visualise_graph(show_fig=False, save_fig=True, save_fig_path=save_fig_path)

def confirm_patch(cve_patches_obj):
    rules = {'src': ['all'], 'searched_entities': ['all'], 'patch_type': ['git_commit', 'svn'],
             'patch_content': ['all']}
    # 提取patch node
    selected_patch_nodes = cve_patches_obj.url_graph.get_patches_by_rules(rules=rules)
    candidate_patch_nodes = []
    priority_rules = None  # 'DG&DP' 'DG&C' 'DG':degree 'C':connectivity 'DP':depth
    if priority_rules:
        cve_patches_obj.url_graph.build_graph()  # 以防未加载 G信息
        # 基于priority选择patch node
        node_priority_info, node_priority_list = cve_patches_obj.url_graph.compute_the_priority_of_nodes(
            nodes=selected_patch_nodes, rules=priority_rules)
        if node_priority_list:
            max_priority = max(node_priority_list)
            candidate_patch_nodes = [node for node in node_priority_info if node_priority_info[node] >= max_priority]
    else:
        candidate_patch_nodes = selected_patch_nodes
    return {CVEID: candidate_patch_nodes}

def confirm_patch(cve_patches_obj):
    rules = {'src': ['all'], 'searched_entities': ['all'], 'patch_type': ['git_commit', 'svn'],
             'patch_content': ['all']}
    # 提取patch node
    selected_patch_nodes = cve_patches_obj.url_graph.get_patches_by_rules(rules=rules)
    candidate_patch_nodes = []
    priority_rules = None  # 'DG&DP' 'DG&C' 'DG':degree 'C':connectivity 'DP':depth
    if priority_rules:
        cve_patches_obj.url_graph.build_graph()  # 以防未加载 G信息
        # 基于priority选择patch node
        node_priority_info, node_priority_list = cve_patches_obj.url_graph.compute_the_priority_of_nodes(
            nodes=selected_patch_nodes, rules=priority_rules)
        if node_priority_list:
            max_priority = max(node_priority_list)
            candidate_patch_nodes = [node for node in node_priority_info if node_priority_info[node] >= max_priority]
    else:
        candidate_patch_nodes = selected_patch_nodes
    return {CVEID: candidate_patch_nodes}

import datetime
from backports.datetime_fromisoformat import MonkeyPatch
MonkeyPatch.patch_fromisoformat()

def extend_github_commit_node(candidate_patch_nodes,cve_patches_obj):
    CVEID = cve_patches_obj.CVEID
    for candidate_patch_node in candidate_patch_nodes:
        # 排除不符合条件的commit
        formatted_url = candidate_patch_node.formatted_url
        if candidate_patch_node.category_in_type != 'git_commit': continue
        if 'github.com' not in formatted_url or '/commit' not in formatted_url: continue
        # formatted_url = 'https://github.com/rails/rails/commit/8a39f411dc3c806422785b1f4d5c7c9d58e4bf85'
        # 获取commit基本信息
        # 'https://api.github.com/repos/apache/hadoop/commits/641d8856d203fc74aac587140a04b0efd5597fc3'
        # formatted_url = 'https://github.com/yarnpkg/yarn/pull/7393/commits/767af6d9558c43c901cf5f46e76f2b5de369c376'
        commit_content = parse_github_commit_util.get_commit_metadata(url=formatted_url)
        if not commit_content: print('commit_content wrong!');continue;
        date = commit_content['commit']['committer']['date']
        message = commit_content['commit']['message']
        owner = commit_content['url'].split('repos/')[-1].split('/')[0]
        repo = commit_content['url'].split('repos/')[-1].split('/')[1]
        # 获取branch信息
        branches_content = github_util.list_branches_of_a_repo( owner=owner, repo=repo ,force_update=True )
        branches =  [ ele['name']  for ele in branches_content ]
        print('branches',branches)
        for branch in branches:
            # # todo, 先暂时舍弃
            # if '/' in branch: continue
            # 获取其他branch上的commit，并进行过滤
            if branch == 'master':
                logger_sprcial_info.info( CVEID + ',' + 'master branch, to check!' )
                print(branch)
            date_day_3_pre = datetime.datetime.fromisoformat(date.strip('Z')) - datetime.timedelta(days = 30)
            date_day_3_pre = date_day_3_pre.isoformat()
            date_day_3_later = datetime.datetime.fromisoformat(date.strip('Z')) + datetime.timedelta(days = 30)
            date_day_3_later = date_day_3_later.isoformat()
            commits_content = github_util.list_commits_with_parameters(owner=owner, repo=repo, branch=branch, since=date_day_3_pre.split('T')[0]+'T', until=date_day_3_later.split('T')[0]+'T' , force_update=True)
            if len(commits_content) > 100:
                logger_sprcial_info.info(CVEID + ',' + 'len(commits_content) > 100, to check!')
            for commits_item in commits_content:
                candi_commit = commits_item['html_url']
                candi_message = commits_item['commit']['message']
                if candi_message == message or candi_message in message or message in candi_message or CVEID in candi_message:
                    # 建 commit node, 并放入图中
                    patch_node_obj = cls_node.Node(node_content_type='url', node_content=candi_commit)
                    cve_patches_obj.url_graph.add_edge(parent_node=candidate_patch_node, child_node=patch_node_obj,
                                                           edge_description='EX')

def start(CVEID):
    print('PID_tool_main_start:' + str(os.getpid()) + ':PID_end')
    print('start(CVEID): ', CVEID)
    start_time = time.time()
    # step1: 初始化CVE对象
    cve_patches_obj = cls_cve_localized_patch.CVELocalizedPatch(CVEID=CVEID, sourses=['N', 'R', 'D', 'G'])
    # cve_patches_obj = cls_cve_localized_patch.CVELocalizedPatch(CVEID=CVEID, sourses=['N', 'R', 'D'])
    step1_time = time.time()
    print(CVEID, 'init_time: step1_time - start_time: ', step1_time - start_time, 'create cve_patches_obj')

    # step2: 读取NVD、Redhat、Debian信息,并解析出refs, 并放入graph中
    src_refs = read_NRD_reports_ref(CVEID, cve_patches_obj)
    step2_time = time.time()
    print(CVEID, 'step1_time: step2_time - step1_time: ', step2_time - step1_time, 'read_NRD_reports_ref')
    for src in src_refs:  # 该两行代码单独提出来，便于布局好看， RDG，在同一水平
        # 建src Node
        src_node_obj = cls_node.Node(node_content_type='source', node_content=src)
        cve_patches_obj.url_graph.add_edge(parent_node=cve_patches_obj.url_graph.root_node, child_node=src_node_obj)

        # step3: 识别 target_types_of_nodes， 放入graph中
        identified_nodes = cls_node.Node.identify_target_types_of_nodes(url_list=src_refs[src], target_types='all')
        for identified_node in identified_nodes:
            cve_patches_obj.url_graph.add_edge(parent_node=src_node_obj, child_node=identified_node, weight=1)
        # cve_patches_obj.url_graph.visualise_graph()
        step3_time = time.time()
        print(CVEID, 'step3_time - step2_time: ', step3_time - step2_time, 'read_NRD_reports_ref')

        # step4: 识别出 nodes to extend, 如：issue node
        nodes_to_extend = [node_ele for node_ele in identified_nodes if
                           node_ele.type not in ['patch_url', 'github_repo_url']]
        count_depth = 2
        while len(nodes_to_extend) and count_depth<4:
            count_depth += 1
            print(CVEID, 'count_depth:', count_depth)
            next_level_nodes_to_extend = []  # 下一层
            for node_ele_to_extend in nodes_to_extend:  # 当前层
                # 循环: step2, step3, step4
                # 分析出 node 网页中的ref
                refs = extract_urls_in_node_website(node_ele_to_extend)
                # 识别target_types_of_nodes, 过滤部分node进行剪枝, 放入graph中
                # identified_nodes = cls_node.Node.identify_target_types_of_nodes(url_list=refs, target_types=['issue_url', 'patch_url', 'github_repo_url'])
                identified_nodes = cls_node.Node.identify_target_types_of_nodes(url_list=refs,
                                                                                target_types=['issue_url', 'patch_url',
                                                                                              'github_repo_url'])
                # print( '11', [ele.formatted_url for ele in identified_nodes])
                identified_nodes = filter_identified_nodes(children_nodes_to_filter=identified_nodes,
                                                           parent_node=node_ele_to_extend)
                # print( '22', [ ele.formatted_url for ele in identified_nodes])
                added_nodes = []
                for identified_node in identified_nodes:
                    add_result = cve_patches_obj.url_graph.add_edge(parent_node=node_ele_to_extend,
                                                                    child_node=identified_node, weight=1)
                    if add_result: added_nodes.append(identified_node)  # 说明时新增的节点，作为后期 nodes_to_extend的范围
                # cve_patches_obj.url_graph.visualise_graph()

                # 识别出 nodes to extend, 如：issue node
                next_level_nodes_to_extend += [node_ele for node_ele in added_nodes if node_ele.type == 'issue_url']
            nodes_to_extend = next_level_nodes_to_extend
            print(CVEID,'!!!!!!!!!!!!!!', len([ele.formatted_id for ele in nodes_to_extend]))
            # cve_patches_obj.url_graph.visualise_graph(show_fig=True, save_fig=False)
        step4_time = time.time()
        print(CVEID, 'step4_time - step3_time: ', step4_time - step3_time)
    # cdb.quit_selenium_crawler()  # 关闭chromedriver, 减少进程开销
    print(CVEID, 'step2&3_time: step4_time - step2_time: ', step4_time - step2_time, 'build url graph')
    print(CVEID, 'step2&3_time: step4_time - start_time: ', step4_time - start_time, 'build url graph')

    # step5: 提取graph中 CVEID、Issuekey、github_repo信息
    node_list = cve_patches_obj.url_graph.nodes
    issueKey_gitrepo_info = extract_and_filter_issueKey_gitrepo_info_in_nodes(node_list)
    # print(CVEID, 'step5: ', issueKey_gitrepo_info)
    step5_time = time.time()
    print(CVEID, 'step4_time: step5_time - step4_time: ', step5_time - step4_time, 'extract_and_filter_issueKey_gitrepo_info_in_nodes')

    # step6: 于Github中搜索commit,并加入图中
    if 'G' in cve_patches_obj.sources:
        search_github_for_fix_result = search_github_for_cve_fix(CVEID=CVEID,
                                                                 issueKey_gitrepo_info=issueKey_gitrepo_info,
                                                                 CVEID_node=cve_patches_obj.url_graph.root_node)
        for ele in search_github_for_fix_result:
            # result_item = { 'input_type': 'issuekey', 'input_node': ele['node_obj'],'input_content': issuekey_str, 'commits': issueKey_search_github_result['git_commit']}
            input_type, input_node, input_content, commits = ele['input_type'], ele['input_node'], ele['input_content'], \
                                                             ele['commits']
            # 建 commit node, 并放入图中
            for commit in commits:
                patch_node_obj = cls_node.Node(node_content_type='url', node_content=commit)
                cve_patches_obj.url_graph.add_edge(parent_node=input_node, child_node=patch_node_obj,
                                                   edge_description='SG')
    step6_time = time.time()
    print(CVEID, 'step5_time: step6_time - step5_time: ', step6_time - step5_time, 'search github')
    # # 可视化
    # save_fig_path = config.PATCH_LOCALICATION_OUTPUT_CVE_GRAPH_PATH % CVEID
    # cve_patches_obj.url_graph.visualise_graph(show_fig=True, save_fig=False, save_fig_path=save_fig_path)

    # confirm
    # candidate_patch_nodes = confirm_patch(cve_patches_obj)[CVEID]
    candidate_patch_nodes = confirm_patch(cve_patches_obj)[CVEID]

    # extend
    extend_github_commit_node( candidate_patch_nodes=candidate_patch_nodes , cve_patches_obj=cve_patches_obj)
    step7_time = time.time()
    print(CVEID, 'extend_github: step7_time - step65_time: ', step7_time - step6_time, 'extend_github_commit_node')
    print(CVEID, 'select&extend_github: step7_time - step6_time: ', step7_time - step6_time, 'select&extend_github_commit_node')
    # # step7: github commit extension
    """
        1. 获取已有的所有github commit（经过过滤的吧， 不然太多了）
        2. 获取该commit metadate，（时间、仓库啥的）
        3. search commits in other branch
        4. filter out candidate patches 
    """


    # step7: 保存运行结果
    cve_patches_obj.url_graph.build_graph()# 存一下图信息
    write_cve_patch_result(cve_patches_obj)
    # step8_time = time.time()
    # print(CVEID, 'step8_time - step7_time: ', step8_time - step7_time, 'write_cve_patch_result')

    # cve_patches_obj = read_cve_patch_result(CVEID)
    # # 可视化
    # save_fig_path = config.PATCH_LOCALICATION_OUTPUT_CVE_GRAPH_PATH % CVEID
    # cve_patches_obj.url_graph.visualise_graph(show_fig=True, save_fig=False, save_fig_path=save_fig_path)
    # selected_patch_nodes = cve_patches_obj.url_graph.get_patches_by_rules()
    # print( [ ele.formatted_url for ele in selected_patch_nodes ]  )
    # visualization_time = time.time()
    # print('visualization_time - step4_time: ', visualization_time - step4_time)
    # print('visualization_time - start_time: ', visualization_time - start_time)

    end_time = time.time()
    print(CVEID, 'finished, consuming time: ', end_time - start_time)
    # evaluate.generate_localized_CVE_patch_result(CVEID=CVEID)  # 单纯为了缓存一下commit信息

def generate_localized_CVE_patch_result_with_rules(CVEID, rules):
    """
    rules = {'src': ['all'], 'priority': 'CN', 'select_add_SG_N': True , 'Extension': 30 , 'searched_entities': ['all'], 'patch_type': ['git_commit', 'svn'], 'patch_content': ['only_code_change','cutoff=4']}  # valid_patch_url ,'only_code_change', 'patch_date', 'only_target_CPEs',
    src: ['all'] , ['withoutR'], ['withoutD'], ['withoutN'], ['withoutG']
    priority: None, 'CN', 'DP', 'DG' (toadd CN不同算法的选项)
    select_add_SG_N: True / False
    Extension: None, int(days)

    :param CVEID:
    :param rules:
    :return:
    """
    # 初始化
    cve_patches_obj = read_cve_patch_result(CVEID)
    # cve_patches_obj.url_graph.compute_graph() # for debug
    # rules = {'src': ['all'], 'priority': 'CN', 'Extension': 30 , 'searched_entities': ['all'], 'patch_type': ['git_commit', 'svn'], 'patch_content': ['only_code_change','cutoff=4']}  # valid_patch_url ,'only_code_change', 'patch_date', 'only_target_CPEs',
    # 提取符合条件的所有patch node
    cve_patches_obj.url_graph.build_graph()  # 以防未加载 G信息
    # cve_patches_obj.url_graph.visualise_graph(show_fig=True)
    if rules['src'] == ['withoutN']:
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='N')
    elif rules['src'] == ['withoutR']:
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='R')
    elif rules['src'] == ['withoutD']:
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='D')
    elif rules['src'] == ['withoutG']:
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='SG')
    elif sorted(rules['src']) == sorted(['withoutR', 'withoutD']):
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='D')
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='R')
    elif sorted(rules['src']) == sorted(['withoutG', 'withoutR', 'withoutD']):
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='SG')
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='D')
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='R')
    elif sorted(rules['src']) == sorted(['withoutN', 'withoutR', 'withoutD']):
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='N')
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='D')
        cve_patches_obj.url_graph = cve_patches_obj.url_graph.generate_new_url_graph_without_src(src='R')

    # cve_patches_obj.url_graph.build_graph()  # 以防未加载 G信息
    if CVEID not in cve_patches_obj.url_graph.node_id_list:  # 甚至连CVE节点都没有了
        print(CVEID, 'no patch')
        return {CVEID: []}
    selected_patch_nodes = cve_patches_obj.url_graph.get_patches_by_rules(rules=rules)
    selected_patch_ids = [ele_node.formatted_id for ele_node in selected_patch_nodes]

    # 先过滤出 EX patch
    # cve_patches_obj.url_graph.print_root()
    non_ex_patch_nodes = cve_patches_obj.url_graph.filter_out_only_EX_patch_nodes(nodes=selected_patch_nodes)
    url_graph_without_EX = cve_patches_obj.url_graph.generate_new_url_graph_without_EX()

    candidate_patch_nodes = []
    priority_rules = rules[
        'priority']  # 'DG&DP' 'DG&C' 'DG':degree 'C':approx.local_node_connectivity 'DP':depth ‘CN‘:'connectivity'
    if priority_rules:
        cve_patches_obj.url_graph.build_graph()  # 以防未加载 G信息
        cutoff_num = 4  # 提取rule中的cutoff数值
        for ele in rules['patch_content']:
            if 'cutoff' in ele:
                cutoff_num = int(ele.split('=')[-1])
        # 基于priority选择patch node
        node_priority_info, node_priority_list = url_graph_without_EX.compute_the_priority_of_nodes(
            nodes=non_ex_patch_nodes, rules=priority_rules, CN_cutoff=cutoff_num)
        # 获得priority_list
        if node_priority_list:
            max_priority = max(node_priority_list)
            candidate_patch_nodes = [node for node in node_priority_info if node_priority_info[node] >= max_priority]
            # 增加一项~, SG + NVD_ref_patch
            if rules['select_add_SG_N']:
                candidate_patch_nodes.extend(
                    url_graph_without_EX.get_NVD_ref_and_SG_patch_node(nodes=non_ex_patch_nodes))
    elif not priority_rules and  rules['select_add_SG_N']:
        candidate_patch_nodes.extend(
            url_graph_without_EX.get_NVD_ref_and_SG_patch_node(nodes=non_ex_patch_nodes))
    else:
        candidate_patch_nodes = non_ex_patch_nodes
    selected_patch_nodes = []
    selected_patch_nodes.extend(candidate_patch_nodes)  # 防止直接是对象地址赋值，就会比较尴尬
    selected_patch_nodes = list(set(selected_patch_nodes))
    # 增加 ‘EX' 的patch
    EXtension_rule = rules['Extension']
    if EXtension_rule or EXtension_rule == 0:
        candidate_patch_nodes = list(set(candidate_patch_nodes))
        candidate_patch_nodes = list(set(candidate_patch_nodes))
        selected_patch_nodes.extend(candidate_patch_nodes)
        for can_patch_node in candidate_patch_nodes:
            # EX_child_nodes = cve_patches_obj.url_graph.get_direct_and_indirect_nodes_with_EX(can_patch_node)
            EX_child_nodes = cve_patches_obj.url_graph.get_direct_nodes_with_EX(can_patch_node,
                                                                                days_span=rules['Extension'], valid_expansion_message=rules['valid_expansion_message'])
            for EX_child_node in EX_child_nodes:
                if EX_child_node.formatted_id in selected_patch_ids:  # 说明是满足patch选取rule的
                    selected_patch_nodes.append(EX_child_node)
                    # candidate_patch_nodes.extend( EX_child_nodes )
        selected_patch_nodes = list(set(selected_patch_nodes))

    if 'limited_patch_num' in rules.keys() and rules['limited_patch_num']:
        if len(selected_patch_nodes) > rules['limited_patch_num']:
            selected_patch_nodes = []
    return {CVEID: selected_patch_nodes}


def main(CVEID, fig_path):
    # CVEID = 'CVE-2021-0051'  # CVE-2017-11555 'CVE-2014-4611' 'CVE-2016-6298' 'CVE-2016-10127'  'CVE-2009-0217' 'CVE-2018-1000169' 'CVE-2010-0156'
    # generate network
    start(CVEID)
    cve_patches_obj = read_cve_patch_result(CVEID)
    cve_patches_obj.url_graph.visualise_graph(save_fig=True, save_fig_path=fig_path)
    cve_patches_obj.url_graph.visualise_graph(show_fig=True)

    # get results
    rules = {'src': ['all'], 'priority': 'CN', 'select_add_SG_N': True, 'Extension': 30,
             'searched_entities': ['all'], 'limited_patch_num': False, 'valid_expansion_message': True,
             'patch_type': ['git_commit', 'svn'], 'patch_content': ['only_code_change',
                                                                    'cutoff=4']}
    patches_in_node = generate_localized_CVE_patch_result_with_rules(CVEID, rules)[CVEID]
    print()
    print('##### OUTPUT #####')
    print('The localized patches for '+CVEID+' are: ')
    for ele in patches_in_node:
        print(ele.formatted_url)

    print()
    print('The network picture has been stored. Path: ' + fig_path)

if __name__ == '__main__':
    CVEID = 'CVE-2017-16016'
    network_pic_path = './'+CVEID + '.pdf'

    CVEID = sys.argv[1]
    network_pic_path = sys.argv[2]
    main(CVEID, network_pic_path)





