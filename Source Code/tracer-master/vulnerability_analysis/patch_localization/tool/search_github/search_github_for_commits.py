#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
-----------------------------------------
@Created: 2020/09/14
------------------------------------------
@Modify: 2020/09/14
------------------------------------------
@Description:

用于实现基于Github repo，搜索patch

real:
0。 根据CVEID，在GitHub中搜索 commit 以及 issue ； 拼接并保存搜索结果
1。 根据 commit / issue 的返回列表，提取repos full names + description；
2。 将 repos name等信息与CVE-CPE vendor product进行相似度计算；并进行最低相似分数控制，获取分数最高的target_github_repo
3。 获得target_github_repo的commit / issue ， 返回commit / issue列表，

copy from: DR010/search_github_repo/search_github_for_commits.py
"""
import os, sys, inspect

current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.append('../../')
sys.path.append('../../..')
sys.path.append('../../../..')

import json
import time
import logging
# from vulnerability_analysis.DR008_present_CVE_metadata import query_CVE_metadata
from vulnerability_analysis.utility import json_processing, log, item_template, file_processing , string_similarity_util, github_util, parse_github_commit_util
from vulnerability_analysis import config
from vulnerability_analysis.utility import log, json_processing
from vulnerability_analysis.utility.NVD import CVE_metadata_util
from vulnerability_analysis.utility.crawler import crawl_util

logger = log.create_logger(level = logging.DEBUG, log_file_path = config.LOG_SEARCH_GITHUB_FOR_COMMITS_PATH, logger_name = 'search_github_for_commits', print_stream_cmd=True)
log_content = file_processing.read_TXTfile(config.LOG_SEARCH_GITHUB_FOR_COMMITS_PATH)

l1_split_str = config.L1_SPLITE_STR

DR010_SEACH_GITHUB_COMMITS_DIR = config.DR010_SEACH_GITHUB_COMMITS_DIR
DR010_SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH = config.DR010_SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH
# repo_similarity_threshold_value = 0.7
# owner_similarity_threshold_value = 0.5
repo_similarity_threshold_value = 2/3
owner_similarity_threshold_value = 2/3

search_github_for_commits_data = {'commit_or_diff_ref': [],
                        'CVEIDs_with_issue_or_commit':[]}

search_github_for_commits_data_path = config.DR010_SEARCH_GITHUB_FOR_COMMITS_DATA_PATH
if file_processing.pathExist(search_github_for_commits_data_path) :
    try:
        search_github_for_commits_data = json_processing.read(search_github_for_commits_data_path)
    except json.decoder.JSONDecodeError as e:
        logger.debug('READ ERROR :' + e.__str__())


def collect_basic_info(CVEID):
    CVE_item_content = CVE_metadata_util.main_no_present(CVEID)
    if CVE_item_content == {}:
        VP = []
    else:
        VP = CVE_item_content['AffectedVendorProductCPE']
    return VP

# 0。 根据CVEID，在GitHub中搜索 commit 以及 issue ； 拼接并保存搜索结果
def query_github_for_commits(CVEID) -> None:
    SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH = DR010_SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH % CVEID
    # 判断已爬取否
    if file_processing.pathExist( SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH  ):
        return

    response_page1 = request_and_judge_response(CVEID, 1)  # 待后期封装
    total_count = response_page1['total_count']
    print(total_count)
    if total_count > 999:
        logger.info( CVEID + 'more than 1k!')
        print(CVEID , 'more than 1k!')
        total_count = 999
        # return

    index = 1
    while total_count / 100 > 1:
        total_count = total_count - 100
        index = index + 1
        # 判断是否已爬取
        page_path = DR010_SEACH_GITHUB_COMMITS_DIR + CVEID + '/' + 'page%s.json' % str(index)
        if file_processing.pathExist(page_path):
            continue
        else:
            request_and_judge_response(CVEID, index)

    # 整合所有page内容
    cve_dir = CVEID
    page_list = file_processing.walk_L1_FileNames(DR010_SEACH_GITHUB_COMMITS_DIR + cve_dir)
    gathered_page_content = None
    # gathered_page_content
    for page in page_list:
        if not page.startswith('page'): continue

        page_content = json_processing.read(DR010_SEACH_GITHUB_COMMITS_DIR + cve_dir + '/' + page)
        if not gathered_page_content:
            gathered_page_content = page_content
        else:
            gathered_page_content[CVEID]['items'].extend(page_content[CVEID]['items'])
    if gathered_page_content:
        json_processing.write(path=SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH,  json_content= gathered_page_content)

# 1。 根据 commit / issue 的返回列表，提取repos full names + description；
def extract_repos_metadata(CVEID):
    repos_name = []
    gathered_page_content = json_processing.read(DR010_SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH % CVEID)
    for record_item in gathered_page_content[CVEID]['items']:
        repo_full_name = record_item['repository']['full_name']
        repo_desc = record_item['repository']['full_name']
        repos_name.append(repo_full_name)
    return list( set(repos_name) )
    # return repos_name

# 2。 将 repos name等信息与CVE-CPE vendor product进行相似度计算；并进行最低相似分数控制，获取分数最高的target_github_repo
def select_target_github_repo(query_product_name, potential_repos):
    candidate_repos = []
    similarity_scores = []
    # max = -1
    # max_index = None
    # for index in range(len(potential_repos)):
    #     potential_repo = potential_repos[index]
    #     similarity_score = string_similarity_util.AVG_similary_from_all(query_product_name, potential_repo.lower().split('/')[1])
    #     similarity_scores.append(similarity_score)
    #     if similarity_score > max:
    #         max = similarity_score
    #         max_index = index
    #         print(potential_repo, similarity_score)
    # print(similarity_scores)
    # print(potential_repos)
    # print(max_index, potential_repos[max_index])
    for index in range(len(potential_repos)):
        potential_repo = potential_repos[index]
        # similarity_score = string_similarity_util.AVG_similary_from_all(query_product_name.split(':')[1], potential_repo.lower().split('/')[1])
        # similarity_score = string_similarity_util.compare_matched_charaters(query_product_name.split(':')[1],potential_repo.lower().split('/')[1])
        similarity_score = string_similarity_util.compare_matched_words(query_product_name.lower(),potential_repo.lower())

        similarity_scores.append(similarity_score)
        # 1. repo / package达到相似度分数要求 repo_similarity_threshold_value
        if similarity_score > repo_similarity_threshold_value:
            print("repo similarity: ",potential_repo, similarity_score)
            # 2. owner / wendor达到相似度分数要求 owner_similarity_threshold_value
            # similarity_score_owner = string_similarity_util.AVG_similary_from_all(query_product_name.split(':')[0], potential_repo.lower().split('/')[0])
            # similarity_score_owner = string_similarity_util.compare_matched_charaters(query_product_name.split(':')[0], potential_repo.lower().split('/')[0])
            similarity_score_owner = string_similarity_util.compare_matched_words(query_product_name.lower(),potential_repo.lower())

            if similarity_score_owner > owner_similarity_threshold_value:
                print("owner similarity: " , potential_repo, similarity_score_owner)
                candidate_repos.append(potential_repo)
    return candidate_repos

# 3。 获得target_github_repo的commit / issue ， 返回commit / issue列表，
def get_commits_from_target_repo(target_github_repos,CVEID):
    result_commits = []
    gathered_page_content = json_processing.read(DR010_SEACH_GITHUB_COMMITS_CVE_GATHERED_CONTENT_PATH % CVEID)
    for record_item in gathered_page_content[CVEID]['items']:
        repo_full_name = record_item['repository']['full_name']
        html_url = record_item['html_url']
        if repo_full_name in target_github_repos:
            result_commits.append(html_url)

    return list(set(result_commits))

def request_and_judge_response(cve_id, page_num):
    print(cve_id)
    time.sleep(10)  # 防止爬虫频率过高
    url = 'https://api.github.com/search/commits?q=' + cve_id + '&page=' + str(page_num) + '&per_page=100'
    # request and save
    while True:
        try:
            response = crawl_util.request_url(url)
            if response:
                logger.debug(response.headers['X-RateLimit-Limit'])
                logger.debug(response.headers['X-RateLimit-Remaining'])
                break
            else:  # 爬虫出错
                print('Try again')
                time.sleep(300)

        except:
            print('Try again')
            time.sleep(300)
            continue

    issue_list = {}
    print(response.status_code)
    if response.status_code == 200:  # 成功
        response_content = json.loads(response.text)
        # print(response_content)
        issue_list[cve_id] = response_content
        file_processing.creatFolder_IfExistPass(DR010_SEACH_GITHUB_COMMITS_DIR + cve_id)  # 创建cve_id文件夹
        json_processing.write(issue_list, DR010_SEACH_GITHUB_COMMITS_DIR + cve_id +
                              '/page' + str(page_num) + '.json')
        # print(response_content)
        return response_content
    elif response.status_code == 400 or response.status_code == 404 or response.status_code == 500:  # 特殊处理一下
        # 被封处理
        logger.debug('request:' + url + ', status: ' + str(response.status_code))
        logger.debug('sleep 30s')
        time.sleep(30)  # 被封后，停30s再试一次
        response = crawl_util.request_url(url)
        if response.status_code == 200:
            return response.content
        else:
            return False
    else:
        # 被封处理
        logger.debug('request:' + url + ', status: ' + str(response.status_code))
        for count in range(3):
            logger.info('sleeping, ' + count.__str__() + 'mins')
            time.sleep(60 * 10)
        request_and_judge_response(url, page_num)

# 针对单个CVE，进行服务化搜索
def main_CVEID_for_patch_collect(CVEID, force_rerun=False, only_search_local_DB=False):
    """

    :param CVEID:
    :param force_rerun: 不管库中是否有，都重新运行；force_rerun 与 only_search_local_DB不能同时为True；
    :param only_search_local_DB: 仅查看库存是否有，有的话返回结果，没有的话返回空；
    :return:
    """
    collected_patch = None
    Existing_CVEIDs_list_commit = [ele['CVEID'] for ele in search_github_for_commits_data['commit_or_diff_ref']]
    # 不强制rerun，且在库中
    if not force_rerun and CVEID in Existing_CVEIDs_list_commit:
        existing_cveid_index = Existing_CVEIDs_list_commit.index(CVEID)
        collected_patch = search_github_for_commits_data['commit_or_diff_ref'][existing_cveid_index]
    # 不强制rerun，且仅搜索库中结果
    elif not force_rerun and only_search_local_DB: return None
    else:
        VP_CPEs = collect_basic_info(CVEID)
        if not VP_CPEs: return None
        VP_CPEs = [ele.replace('__fdse__', ':') for ele in VP_CPEs]
        # print(VP_CPEs)
        query_github_for_commits(CVEID)
        returned_topN_related_repos = extract_repos_metadata(CVEID)
        CVEID_search_github_result = {'CVEID': CVEID, 'git_commit': []}
        CVEID_search_github_result_with_CPE = {}
        CVEID_search_github_result_with_CPE[CVEID] = {}
        hit_flag = 0
        if len(returned_topN_related_repos) > 0:
            for CPE in VP_CPEs:
                target_github_repos = select_target_github_repo(query_product_name=CPE,
                                                                potential_repos=returned_topN_related_repos)
                print(target_github_repos)
                if len(target_github_repos) > 0:
                    hit_flag = 1
                    result_commits = get_commits_from_target_repo(target_github_repos=target_github_repos, CVEID=CVEID)
                    CVEID_search_github_result['git_commit'].extend(result_commits)
                    CVEID_search_github_result_with_CPE[CVEID].update({CPE: result_commits})
        CVEID_search_github_result['git_commit'] = list(set(CVEID_search_github_result['git_commit']))

        if hit_flag == 1:
            # 判断是否已有该CVEID的数据
            if CVEID in Existing_CVEIDs_list_commit:  # 存在,更新
                existing_cveid_index = Existing_CVEIDs_list_commit.index(CVEID)
                logger.info(
                    'update: ' + search_github_for_commits_data['commit_or_diff_ref'][existing_cveid_index].__str__())
                search_github_for_commits_data['commit_or_diff_ref'][existing_cveid_index] = CVEID_search_github_result

            else:
                search_github_for_commits_data['commit_or_diff_ref'].append(CVEID_search_github_result)
                search_github_for_commits_data['CVEIDs_with_issue_or_commit'].append(CVEID)
            # 更新库信息
            json_processing.write(json_content=search_github_for_commits_data , path=search_github_for_commits_data_path)
            collected_patch = CVEID_search_github_result
    return collected_patch

def main_issueKey_for_patch_collect(CVEID, issuekey_str, force_update=False):
    result = None
    try:
        if force_update:
            commits = github_util.query_github_for_commits(issuekey_str,force_update=True)
        else:
            commits = github_util.query_github_for_commits(issuekey_str)
    except json.decoder.JSONDecodeError:
        commits = github_util.query_github_for_commits(issuekey_str,force_update=True)
    if commits and len(commits) > 0:
        # 将 repos name等信息与CVE-CPE vendor product进行相似度计算; 去除噪声commit
        filtered_commits_issuekey = []
        VP_list = collect_basic_info(CVEID)
        if not VP_list: return None
        VP_CPEs = [ele.replace('__fdse__', ':') for ele in VP_list]
        for ele in commits:
            res_owner = ele.split('/')[-4]
            res_repo = ele.split('/')[-3]
            similarity_score_owner_max = 0
            similarity_score_repo_max = 0
            for VP in VP_CPEs:
                # similarity_score_owner = string_similarity_util.AVG_similary_from_all(res_owner.lower(),VP.lower().split(':')[0])
                # similarity_score_repo = string_similarity_util.AVG_similary_from_all(res_repo.lower(),VP.lower().split(':')[1])
                # similarity_score_owner = string_similarity_util.compare_matched_charaters(res_owner.lower(),VP.lower().split(':')[0])
                # similarity_score_repo = string_similarity_util.compare_matched_charaters(res_repo.lower(),VP.lower().split(':')[1])

                similarity_score_owner = string_similarity_util.compare_matched_words(res_owner + '/' + res_repo.lower(),VP.lower())
                similarity_score_repo = string_similarity_util.compare_matched_words(res_owner + '/' + res_repo.lower(),VP.lower())

                if similarity_score_owner_max < similarity_score_owner: similarity_score_owner_max = similarity_score_owner
                if similarity_score_repo_max < similarity_score_repo: similarity_score_repo_max = similarity_score_repo
            if res_owner == res_repo:  # 则取repo名的值
                similarity_score_owner_max = similarity_score_repo_max
            # filtered_commits_issuekey
            if similarity_score_repo_max >= repo_similarity_threshold_value and similarity_score_owner_max >= owner_similarity_threshold_value:
                commit_content = parse_github_commit_util.get_commit_metadata(url=ele)
                if not commit_content: continue # 说明无有效内容
                commit_message = commit_content['commit']['message']
                if issuekey_str.lower() in commit_message.lower(): # 当出现在message中时，才放入！
                    filtered_commits_issuekey.append(ele)
        if len(filtered_commits_issuekey) > 0:
            result = {'git_commit': list(set(filtered_commits_issuekey)), 'diff_file': [], 'svn': [],'issuekey': issuekey_str}
    return result

def main_issueKey_for_patch_collect_test():
    CVEID  = 'CVE-2016-10149'
    issuekey_str = "123276"
    main_issueKey_for_patch_collect(CVEID, issuekey_str)
    print(CVEID, issuekey_str)

    CVEID = 'CVE-2016-10127'
    issuekey_str = "12896"
    main_issueKey_for_patch_collect(CVEID, issuekey_str)
    print(CVEID, issuekey_str)

if __name__ == '__main__':
    # main_issueKey_for_patch_collect_test()
    collect_basic_info(CVEID='CVE-2017-16016')