# -*-coding:utf-8-*-

"""
 request timeout 设置，https://www.cnblogs.com/gl1573/p/10129382.html

"""
import random
import time
import os, sys, inspect
import traceback
from typing import Tuple

current_dir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))
parent_dir = os.path.dirname(current_dir)
sys.path.insert(0, parent_dir)
sys.path.append('../')
sys.path.append('../..')
sys.path.append('../../..')

import requests
from requests.adapters import HTTPAdapter
from urllib3.exceptions import InsecureRequestWarning

requests.packages.urllib3.disable_warnings(InsecureRequestWarning)

from vulnerability_analysis.utility.crawler import user_agents
from vulnerability_analysis.utility import json_processing, log, file_processing
from vulnerability_analysis import config
import logging

logger = log.create_logger(level=logging.INFO, log_file_path=config.LOG_CRAWL_UTIL, logger_name='crawl_util',
                           print_stream_cmd=True)

request_black_str_list = ['.zip', '.gz', '.tgz', '.google.', '.msi', '.exe', '.tar', '.iso',
                          '.sha256', '.asc', '.xz', '.atom', '.pdf']


# for github
def request_url(url):
    # print('------------------------- url:' + url)
    logger.info('------------------------- url:' + url)
    # 爬虫黑名单判断
    for ele in request_black_str_list:
        if ele in url: return None
    # headers = {'User-Agent': random.choice(user_agents.agents) , "Accept-Language": 'zh-CN,zh;q=0.9,en;q=0.8'}
    # headers = {'User-Agent': random.choice(user_agents.agents), 'Authorization': 'token 919bf2a86f586863413dd26549c8d2331dd7fadd', "Accept-Language": 'q=0.9,en;q=0.8'}
    headers = {'Authorization': 'token 919bf2a86f586863413dd26549c8d2331dd7fadd', "Accept-Language": 'q=0.9,en;q=0.8',
               'Accept': 'application/vnd.github.cloak-preview+json'}
    s = requests.session()
    s.keep_alive = False
    s.mount('http://', HTTPAdapter(max_retries=3))
    s.mount('https://', HTTPAdapter(max_retries=3))

    try:
        res = requests.head(url, timeout=(5, 30))
        if 'Content-Length' in res.headers.keys():
            Content_Length = res.headers['Content-Length']
            if int(Content_Length) > 1000000:  # >1M, 则不进行request
                print('Error: ' + "Target web file's size too large." + Content_Length.__str__())
                logger.info('Error: ' + "Target web file's size too large." + Content_Length.__str__())
                return None
        if res.status_code == 302:  # 302， 临时重定向
            if 'Location' in res.headers.keys():
                location_url = res.headers['Location']
                print('302 redirecttion: ' + location_url.__str__())
                logger.info('302 redirecttion: ' + location_url.__str__())
                return request_url(location_url)
        if 'status' in res.headers.keys():
            status = res.headers['status']
            if status == '302 Found':  # 302， 临时重定向
                print('Error: ' + '302 Found')
                logger.info('Error: ' + '302 Found')
                return None
        url_content = requests.get(url, headers=headers, timeout=(5, 30), verify=False)
    except Exception as e:
        print('Error', e)
        logger.info('Error' + e.__str__())
        return None
    else:
        return url_content


# for common
def request_url_for_common(url):
    # print('------------------------- url:' + url)
    logger.info('------------------------- url:' + url)
    # 爬虫黑名单判断
    for ele in request_black_str_list:
        if ele in url: return None
    headers = {'User-Agent': random.choice(user_agents.agents), "Accept-Language": 'zh-CN,zh;q=0.9,en;q=0.8'}
    # headers = {'User-Agent': random.choice(user_agents.agents), 'Authorization': 'token 919bf2a86f586863413dd26549c8d2331dd7fadd', "Accept-Language": 'q=0.9,en;q=0.8'}
    # headers = {'Authorization': 'token 919bf2a86f586863413dd26549c8d2331dd7fadd', "Accept-Language": 'q=0.9,en;q=0.8' , 'Accept': 'application/vnd.github.cloak-preview+json'}
    s = requests.session()
    s.keep_alive = False
    s.mount('http://', HTTPAdapter(max_retries=3))
    s.mount('https://', HTTPAdapter(max_retries=3))

    try:
        res = requests.head(url, timeout=(5, 30))
        if 'Content-Length' in res.headers.keys():
            Content_Length = res.headers['Content-Length']
            if int(Content_Length) > 4000000:  # >1M, 则不进行request
                print('Error: ' + "Target web file's size too large." + Content_Length.__str__())
                logger.info('Error: ' + "Target web file's size too large." + Content_Length.__str__())
                return None
        if res.status_code == 302:  # 302， 临时重定向
            if 'Location' in res.headers.keys():
                location_url = res.headers['Location']
                print('302 redirecttion: ' + location_url.__str__())
                logger.info('302 redirecttion: ' + location_url.__str__())
                return request_url(location_url)
        if 'status' in res.headers.keys():
            status = res.headers['status']
            if status == '302 Found':  # 302， 临时重定向
                print('Error: ' + '302 Found')
                logger.info('Error: ' + '302 Found')
                return None
        url_content = requests.get(url, headers=headers, timeout=(5, 30), verify=False,
                                   cookies={'__test': '2501c0bc9fd535a3dc831e57dc8b1eb0'})
    except Exception as e:
        print('Error', e)
        logger.info('Error' + e.__str__())
        return None
    else:
        return url_content


def request_url_for_common_simplified(url: str) -> Tuple[int, str]:
    logger.info('------------------------- url:' + url)
    try:
        headers = {'User-Agent': random.choice(user_agents.agents), "Accept-Language": 'zh-CN,zh;q=0.9,en;q=0.8'}
        s = requests.session()
        s.keep_alive = False
        s.mount('http://', HTTPAdapter(max_retries=3))
        s.mount('https://', HTTPAdapter(max_retries=3))
        res = requests.get(url, headers=headers, timeout=(5, 30), verify=False,
                           cookies={'__test': '2501c0bc9fd535a3dc831e57dc8b1eb0'})
        raw = res.content.decode('utf-8')
        status_code = res.status_code
        return status_code, raw
    except requests.exceptions.ConnectTimeout:
        return -2, traceback.format_exc()
    except requests.exceptions.ConnectionError:
        return -3, traceback.format_exc()
    except UnicodeDecodeError:
        return -4, traceback.format_exc()
    except requests.exceptions.ReadTimeout:
        return -5, traceback.format_exc()
    except:
        print('之前没出现过的爬虫报错, 记得来看一下:')
        traceback.print_exc()
        return -1, traceback.format_exc()



def save_file_from_url(url, path):
    if os.path.exists(path):
        return
    time.sleep(random.randint(1, 3))
    headers = {'User-Agent': random.choice(user_agents.agents)}
    package = requests.get(url, headers=headers)
    with open(path, "wb") as f:
        f.write(package.content)
    f.close()


def save_file_from_url_cover_existing(url, path):
    time.sleep(random.randint(1, 3))
    headers = {'User-Agent': random.choice(user_agents.agents)}
    package = requests.get(url, headers=headers)
    # print(path)
    with open(path, "wb") as f:
        f.write(package.content)
    f.close()


def request_and_judge_response(url):
    # request and save
    reponse = request_url(url)
    # reponse = requests.get(url)
    # print(reponse.status_code,',,,,,,,,,,,,,,,')
    if not reponse :  # 检测request结果
        return None
    logger.debug('reponse.status_code:' + reponse.status_code.__str__())
    if reponse.status_code == 200:  # 成功
        reponse_content = reponse.content
        return reponse_content
    elif reponse.status_code == 400 or reponse.status_code == 404 or reponse.status_code == 500 \
            or reponse.status_code == 999 or reponse.status_code == 301 or reponse.status_code == 502:  # 特殊处理一下
        # 被封处理
        logger.info('request:' + url + ', status: ' + str(reponse.status_code))
        logger.info('sleep 30s')
        time.sleep(30)  # 被封后，停30s再试一次
        reponse = request_url(url)
        if reponse.status_code == 200:
            return reponse.content
        else:
            return False
    else:
        # 被封处理
        logger.info('request:' + url + ', status: ' + str(reponse.status_code))
        for count in range(3):
            logger.info('sleeping, ' + count.__str__() + 'mins')
            time.sleep(60 * 1)
        # 1。递归
        # request_and_judge_response(url)

        # 2。推出
        reponse = request_url(url)
        if reponse.status_code == 200:
            return reponse.content
        else:
            return False


# debug Github API OAuth
import json
# # url = 'https://api.github.com/search/issues?q=CVE-2020-9547&page=1&per_page=100&'
# url = 'https://api.github.com/repos/LibreDWG/libredwg/issues/99/comments'
# res = request_url(url)
# json_content = json.loads( res.content )
# json_processing.write( json_content=json_content , path= 'json_content2.json')
# print( res.status_code )
# print( res.headers['X-RateLimit-Limit'], res.headers['X-RateLimit-Remaining'] )
# print( len(json_content['items']) )

# url = 'https://projects.theforeman.org/attachments/855/0001-fixes-6086-CVE-2014-0007-fixed-TFTP-boot-API-remote-.patch'
# url = 'https://lists.apache.org/thread.html/fb6c84fd387de997e5e366d50b0ca331a328c466432c80f8c5eed33d@%3Cdev.tika.apache.org%3E'
# url = 'https://lists.apache.org/api/source.lua/fb6c84fd387de997e5e366d50b0ca331a328c466432c80f8c5eed33d@%3Cdev.tika.apache.org%3E'
# url = 'https://issues.apache.org/jira/browse/SOLR-10624'
# res = request_url_for_common(url)
# file_processing.write_TXTfile('test.html' , content=res.content)

# url = 'https://lists.apache.org/thread.html/fb6c84fd387de997e5e366d50b0ca331a328c466432c80f8c5eed33d@%3Cdev.tika.apache.org%3E'
# url = 'https://issues.apache.org/jira/browse/DERBY-6807'
# response = requests.get(url, cookies={'__test': '2501c0bc9fd535a3dc831e57dc8b1eb0'})
# file_processing.write_TXTfile('test.html' , content=response.content)

# url = 'https://www.bouncycastle.org/download/bctls-jdk15on-166.jar'
# url = 'https://127.0.0.1:6379/?q=HTTP/1.1\r\nSE'
# res = request_url(url)
# print(res)
#
# if __name__ == '__main__':
#     print(1)
#     o = request_url_for_common_simplified('https://bugzilla.redhat.com/describecomponents.cgi?product=Fedora')
#     print(len(o[1]))
#     print(2)

# url = 'https://lists.apache.org/api/source.lua/158ab719cf60448ddbb074798f09152fdb572fc8f781e70a56118d1a@%3Cdev.tomcat.apache.org%3E'
# res = request_url_for_common_simplified(url)
# print(res)
